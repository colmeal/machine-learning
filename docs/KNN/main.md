K-Nearest Neighbors:

O KNN (K-Nearest Neighbors) Ã© um algoritmo de aprendizado supervisionado usado tanto para classificaÃ§Ã£o quanto para regressÃ£o.

ğŸŒŸ Ideia principal

Para prever a classe ou valor de um novo ponto, o KNN procura os K vizinhos mais prÃ³ximos dele no conjunto de treinamento.

A decisÃ£o Ã© baseada nesses vizinhos:

ClassificaÃ§Ã£o: escolhe a classe mais frequente entre os vizinhos.

RegressÃ£o: calcula a mÃ©dia (ou outra medida) dos valores dos  vizinhos.

ğŸ”‘ Passos do algoritmo

Escolher o valor de K (nÃºmero de vizinhos a considerar).

Calcular a distÃ¢ncia entre o novo ponto e todos os pontos do conjunto de treino (normalmente distÃ¢ncia Euclidiana).

Selecionar os K pontos mais prÃ³ximos.

Fazer a previsÃ£o:

Classe mais votada â†’ classificaÃ§Ã£o.

MÃ©dia/mediana â†’ regressÃ£o.

âœ… Vantagens

Simples e intuitivo.

Funciona bem em problemas com fronteiras de decisÃ£o complexas.

âš ï¸ Desvantagens

Custo alto em prediÃ§Ã£o (precisa calcular distÃ¢ncia para todos os pontos).

SensÃ­vel a atributos com escalas diferentes â†’ normalizaÃ§Ã£o dos dados Ã© essencial.

Escolher o valor de K pode ser difÃ­cil (muito pequeno â†’ ruÃ­do; muito grande â†’ perda de detalhes).

ğŸ‘‰ Exemplo rÃ¡pido:
Se K=3 e os trÃªs vizinhos mais prÃ³ximos de um ponto novo forem [â€œGatoâ€, â€œCachorroâ€, â€œGatoâ€], o KNN prevÃª â€œGatoâ€ (maioria).