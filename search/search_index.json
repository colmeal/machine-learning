{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#entregavel-ml","title":"Entregavel ML","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#nome","title":"Nome","text":"<ul> <li>Mateus Colmeal</li> </ul> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"Relatorio_KNN_KMeans/","title":"Relat\u00f3rio do Projeto \u2013 KNN e K-Means","text":""},{"location":"Relatorio_KNN_KMeans/#relatorio-do-projeto-knn-e-k-means","title":"Relat\u00f3rio do Projeto \u2013 KNN e K-Means","text":""},{"location":"Relatorio_KNN_KMeans/#1-exploracao-dos-dados","title":"1. Explora\u00e7\u00e3o dos Dados","text":"<ul> <li>Dataset: <code>fitness_dataset.csv</code> com atributos demogr\u00e1ficos e de estilo de vida.</li> <li>Alvo: <code>is_fit</code> (1 = Fit, 0 = Not Fit).</li> <li>Tratamentos aplicados: substitui\u00e7\u00e3o de <code>'?'</code>, preenchimento por moda, cria\u00e7\u00e3o de <code>bmi</code>,   convers\u00e3o <code>smokes</code>\u2192<code>smokes_bin</code>.</li> </ul>"},{"location":"Relatorio_KNN_KMeans/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<ul> <li>Num\u00e9ricos: StandardScaler.</li> <li>Categ\u00f3ricos: OneHotEncoder.</li> <li>Split estratificado 80/20 (treino/teste).</li> </ul>"},{"location":"Relatorio_KNN_KMeans/#3-knn-supervisionado","title":"3. KNN (supervisionado)","text":"<ul> <li>GridSearchCV (cv=5): <code>n_neighbors</code> 1\u201320, <code>weights</code> ['uniform','distance'], <code>p</code> [1,2].</li> <li>Melhores hiperpar\u00e2metros: {'clf__n_neighbors': 16, 'clf__p': 1, 'clf__weights': 'distance'}</li> <li>M\u00e9tricas (teste): |          |    Valor | |:---------|---------:| | Acur\u00e1cia | 0.74     | | Precis\u00e3o | 0.705882 | | Recall   | 0.6      | | F1       | 0.648649 |</li> </ul>"},{"location":"Relatorio_KNN_KMeans/#4-k-means-nao-supervisionado","title":"4. K-Means (n\u00e3o supervisionado)","text":"<ul> <li>K=2, treino no conjunto de treino; avalia\u00e7\u00e3o no teste ap\u00f3s mapear cluster\u2192classe (maioria no treino).</li> <li>M\u00e9tricas (teste): |          |   Valor | |:---------|--------:| | Acur\u00e1cia |     0.6 | | Precis\u00e3o |     0   | | Recall   |     0   | | F1       |     0   |</li> </ul>"},{"location":"Relatorio_KNN_KMeans/#5-comparacao-dos-resultados","title":"5. Compara\u00e7\u00e3o dos Resultados","text":"Acur\u00e1cia Precis\u00e3o Recall F1 KNN (Supervisionado) 0.74 0.7059 0.6 0.6486 K-Means (N\u00e3o supervisionado) 0.6 0 0 0"},{"location":"Relatorio_KNN_KMeans/#6-conclusoes","title":"6. Conclus\u00f5es","text":"<ul> <li>KNN apresentou melhor desempenho global (Acur\u00e1cia/F1 maiores), como esperado para m\u00e9todo supervisionado.</li> <li>K-Means foi \u00fatil para segmenta\u00e7\u00e3o explorat\u00f3ria, aproximando parcialmente as classes reais.</li> <li>Pr\u00f3ximos passos: balanceamento (SMOTE), ajustes de features e compara\u00e7\u00e3o com Log\u00edstica/SVM/Random Forest.</li> </ul>"},{"location":"relatorio_final_classificacao/","title":"Projeto \u2014 Classifica\u00e7\u00e3o com KNN e K-Means","text":""},{"location":"relatorio_final_classificacao/#projeto-classificacao-com-knn-e-k-means","title":"Projeto \u2014 Classifica\u00e7\u00e3o com KNN e K-Means","text":"<p>Base: <code>docs/data/fitness_dataset.csv</code> Alvo: <code>is_fit</code> (0 = Not Fit, 1 = Fit) Bibliotecas: pandas \u00b7 numpy \u00b7 matplotlib \u00b7 scikit-learn</p>"},{"location":"relatorio_final_classificacao/#1-exploracao-dos-dados-eda","title":"1) Explora\u00e7\u00e3o dos Dados (EDA)","text":"<ul> <li>Estat\u00edsticas descritivas salvas em: <code>data/eda_descritivas.csv</code> </li> </ul>"},{"location":"relatorio_final_classificacao/#2-aplicacao-das-tecnicas","title":"2) Aplica\u00e7\u00e3o das T\u00e9cnicas","text":"<p>KNN (GridSearchCV) \u2192 melhor: <code>{\"knn__n_neighbors\": 17, \"knn__p\": 2, \"knn__weights\": \"distance\"}</code> </p> <p>K-Means ajustado com K = 3 (hip\u00f3tese de tr\u00eas perfis).</p> <p></p>"},{"location":"relatorio_final_classificacao/#3-matrizes-de-confusao","title":"3) Matrizes de Confus\u00e3o","text":"<p>KNN (teste):</p> <p></p> <p>K-Means vs Classe real:</p> <p></p>"},{"location":"relatorio_final_classificacao/#4-avaliacao-dos-modelos","title":"4) Avalia\u00e7\u00e3o dos Modelos","text":"<p>KNN (classe positiva = Fit = 1):</p> <ul> <li>Accuracy: 0.7325 </li> <li>Precision (1): 0.7054 </li> <li>Recall (1): 0.5687 </li> <li>F1 (1): 0.6298 </li> <li>F1 Macro: 0.7102 </li> <li>Balanced Accuracy: 0.7052 </li> </ul> <p></p> <p>K-Means (coer\u00eancia externa):</p> <ul> <li>ARI: 0.0557 </li> <li>Homogeneity: 0.0692 </li> <li>Completeness: 0.0432 </li> <li>V-Measure: 0.0532 </li> </ul>"},{"location":"relatorio_final_classificacao/#5-comparacao-dos-resultados","title":"5) Compara\u00e7\u00e3o dos Resultados","text":"Algoritmo M\u00e9trica principal Valor Observa\u00e7\u00e3o KNN (sup.) Accuracy 0.7325 Bom preditor direto de is_fit; AUC 0.80 K-Means (n\u00e3o sup.) ARI 0.0557 Agrupa perfis; coer\u00eancia moderada (V-Measure 0.05)"},{"location":"relatorio_final_classificacao/#6-documentacao","title":"6) Documenta\u00e7\u00e3o","text":"<ul> <li> <p>C\u00f3digo: <code>run_projeto_classificacao.py</code> </p> </li> <li> <p>Figuras geradas em <code>docs/img/</code> e tabelas em <code>data/</code> </p> </li> <li> <p>Este relat\u00f3rio foi gerado automaticamente.</p> </li> </ul>"},{"location":"K-Mean/main/","title":"K-Mean","text":""},{"location":"K-Mean/main/#projeto-de-machine-learning-k-means","title":"Projeto de Machine Learning \u2013 K-Means","text":""},{"location":"K-Mean/main/#1-exploracao-dos-dados","title":"1. Explora\u00e7\u00e3o dos Dados","text":"<p>O dataset analisado, <code>fitness_dataset.csv</code>, cont\u00e9m 2.000 registros e 11 vari\u00e1veis, relacionadas \u00e0 sa\u00fade e h\u00e1bitos de vida dos participantes. O objetivo da an\u00e1lise foi aplicar K-Means para identificar agrupamentos naturais (clusters) de indiv\u00edduos com caracter\u00edsticas semelhantes, sem utilizar a vari\u00e1vel-alvo <code>is_fit</code>.</p>"},{"location":"K-Mean/main/#descricao-das-variaveis-principais","title":"Descri\u00e7\u00e3o das vari\u00e1veis principais","text":"Vari\u00e1vel Descri\u00e7\u00e3o Tipo <code>age</code> Idade do indiv\u00edduo Num\u00e9rica <code>height_cm</code> Altura em cent\u00edmetros Num\u00e9rica <code>weight_kg</code> Peso corporal (com outliers) Num\u00e9rica <code>heart_rate</code> Frequ\u00eancia card\u00edaca de repouso Num\u00e9rica <code>blood_pressure</code> Press\u00e3o arterial sist\u00f3lica Num\u00e9rica <code>sleep_hours</code> M\u00e9dia de horas de sono Num\u00e9rica (com NaN) <code>nutrition_quality</code> Qualidade da alimenta\u00e7\u00e3o (0\u201310) Num\u00e9rica <code>activity_index</code> N\u00edvel de atividade f\u00edsica (1\u20135) Num\u00e9rica <code>smokes</code> H\u00e1bito de fumar (0/1 ou sim/n\u00e3o) Categ\u00f3rica <code>gender</code> Sexo biol\u00f3gico Categ\u00f3rica <code>is_fit</code> Alvo bin\u00e1rio (0 = n\u00e3o em forma, 1 = em forma) Bin\u00e1ria"},{"location":"K-Mean/main/#analise-exploratoria","title":"\ud83d\udcca An\u00e1lise explorat\u00f3ria","text":"<ul> <li>A vari\u00e1vel <code>is_fit</code> apresenta leve desbalanceamento (~60% n\u00e3o fit e 40% fit).  </li> <li>H\u00e1 correla\u00e7\u00f5es coerentes:  </li> <li><code>activity_index</code> e <code>nutrition_quality</code> correlacionam-se positivamente com o fitness;  </li> <li><code>weight_kg</code>, <code>heart_rate</code> e <code>blood_pressure</code> est\u00e3o associadas negativamente.  </li> <li>O dataset possui ru\u00eddo e sobreposi\u00e7\u00e3o entre classes, simulando dados reais.</li> </ul> <p>(Insira aqui figuras explorat\u00f3rias geradas: distribui\u00e7\u00e3o, crosstab por g\u00eanero, estat\u00edsticas descritivas.)</p>"},{"location":"K-Mean/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>O tratamento dos dados incluiu:</p> <ol> <li>Limpeza e imputa\u00e7\u00e3o de valores ausentes</li> <li>S\u00edmbolos \u201c?\u201d convertidos em <code>NaN</code>.</li> <li>Num\u00e9ricas: imputadas pela mediana.</li> <li> <p>Categ\u00f3ricas: imputadas pela moda.</p> </li> <li> <p>Padroniza\u00e7\u00e3o e codifica\u00e7\u00e3o</p> </li> <li><code>smokes</code> convertida em bin\u00e1ria (<code>yes/1</code> \u2192 1, <code>no/0</code> \u2192 0).  </li> <li> <p><code>gender</code> transformada em OneHotEncoder.</p> </li> <li> <p>Cria\u00e7\u00e3o de vari\u00e1vel derivada</p> </li> <li> <p><code>bmi</code> (\u00edndice de massa corporal) = <code>weight_kg / (height_cm/100)**2</code>.</p> </li> <li> <p>Normaliza\u00e7\u00e3o</p> </li> <li>Escalonamento com StandardScaler para evitar distor\u00e7\u00e3o entre vari\u00e1veis.</li> </ol> <p>Trecho ilustrativo: </p><pre><code>from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_imputer = SimpleImputer(strategy=\"median\")\nX_num = num_imputer.fit_transform(X_num_raw)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_num)\n</code></pre><p></p>"},{"location":"K-Mean/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>Abordagem adotada:</p> <ul> <li>O K\u2011Means \u00e9 n\u00e3o supervisionado, portanto o treino usa todos os registros processados.  </li> <li>Entretanto, para avalia\u00e7\u00e3o e relat\u00f3rios, preservei a coluna <code>is_fit</code> e exportei atribui\u00e7\u00f5es por registro para valida\u00e7\u00e3o externa.</li> </ul> <p>Arquivos gerados: - <code>data/kmeans_assignments.csv</code> \u2014 atribui\u00e7\u00e3o de cluster por registro com <code>is_fit</code>. - <code>data/kmeans_cluster_profile.csv</code> \u2014 m\u00e9dias por cluster. - <code>data/kmeans_cluster_fit_distribution.csv</code> \u2014 propor\u00e7\u00e3o de <code>is_fit</code> por cluster.</p>"},{"location":"K-Mean/main/#4-treinamento-e-execucao-do-k-means","title":"4. Treinamento e Execu\u00e7\u00e3o do K-Means","text":"<p>O modelo foi implementado conforme o pipeline:</p> <pre><code>from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X_scaled)\n</code></pre>"},{"location":"K-Mean/main/#projecao-pca-2d-dos-clusters","title":"\ud83d\udd39 Proje\u00e7\u00e3o PCA 2D dos Clusters","text":"<p>A redu\u00e7\u00e3o de dimensionalidade com PCA exibe tr\u00eas agrupamentos principais: - Dois clusters com alta sobreposi\u00e7\u00e3o (h\u00e1bitos intermedi\u00e1rios). - Um cluster mais distinto, representando casos extremos (ex.: alto BMI ou baixa atividade).</p>"},{"location":"K-Mean/main/#5-avaliacao-e-interpretacao-dos-resultados","title":"5. Avalia\u00e7\u00e3o e Interpreta\u00e7\u00e3o dos Resultados","text":""},{"location":"K-Mean/main/#perfis-medios-por-cluster","title":"Perfis m\u00e9dios por cluster","text":"Cluster Perfil predominante Caracter\u00edsticas m\u00e9dias observadas 0 \u2013 Sedent\u00e1rio Maior peso, menor atividade e nutri\u00e7\u00e3o <code>activity_index</code> baixo, <code>bmi</code> alto, <code>nutrition_quality</code> baixo 1 \u2013 Intermedi\u00e1rio H\u00e1bitos medianos Valores equilibrados 2 \u2013 Ativo/Saud\u00e1vel Melhor nutri\u00e7\u00e3o e atividade f\u00edsica <code>activity_index</code> e <code>nutrition_quality</code> altos, <code>bmi</code> baixo"},{"location":"K-Mean/main/#cruzamento-com-is_fit","title":"\ud83d\udcc8 Cruzamento com <code>is_fit</code>","text":"<ul> <li>Cluster 2 (Ativo) \u2192 maior propor\u00e7\u00e3o de <code>is_fit = 1</code>.  </li> <li>Cluster 0 (Sedent\u00e1rio) \u2192 predomin\u00e2ncia de <code>is_fit = 0</code>.  </li> <li>Cluster 1 (Intermedi\u00e1rio) \u2192 mistura equilibrada entre ambos.</li> </ul> <p>Essas rela\u00e7\u00f5es confirmam que o modelo capturou padr\u00f5es coerentes com condicionamento f\u00edsico, mesmo sem usar <code>is_fit</code> no treino.</p>"},{"location":"K-Mean/main/#6-relatorio-final-e-melhorias","title":"6. Relat\u00f3rio Final e Melhorias","text":""},{"location":"K-Mean/main/#conclusoes","title":"\ud83e\uddfe Conclus\u00f5es","text":"<ul> <li>K-Means com K = 3 apresentou boa interpretabilidade pr\u00e1tica e capturou perfis coerentes de sa\u00fade.  </li> <li>Resultados mostram rela\u00e7\u00e3o consistente entre atividade, nutri\u00e7\u00e3o e probabilidade de estar em forma.</li> </ul>"},{"location":"K-Mean/main/#limitacoes","title":"\u26a0\ufe0f Limita\u00e7\u00f5es","text":"<ul> <li>Outliers em <code>weight_kg</code> e <code>blood_pressure</code> afetam a forma\u00e7\u00e3o dos clusters.</li> </ul>"},{"location":"K-Mean/main/#sugestoes-de-melhoria","title":"\ud83d\ude80 Sugest\u00f5es de melhoria","text":"<ol> <li>Testar algoritmos alternativos (DBSCAN, Gaussian Mixture).  </li> <li>Tratar/remover outliers em <code>weight_kg</code> e <code>bmi</code> antes do agrupamento.  </li> <li>Aplicar PCA ou t-SNE antes do K\u2011Means para melhorar separabilidade.  </li> <li>Enriquecer o conjunto de features (IMC categ\u00f3rico, qualidade do sono categorizada, intera\u00e7\u00f5es).</li> </ol>"},{"location":"KNN/main/","title":"KNN","text":""},{"location":"KNN/main/#relatorio-do-projeto-de-machine-learning-knn","title":"Relat\u00f3rio do Projeto de Machine Learning \u2013 KNN","text":""},{"location":"KNN/main/#1-exploracao-dos-dados","title":"1. Explora\u00e7\u00e3o dos Dados","text":"<p>Descri\u00e7\u00e3o - Dataset: <code>fitness_dataset.csv</code> com atributos demogr\u00e1ficos e de estilo de vida (age, weight_kg, activity_index, smokes, etc.). - Vari\u00e1vel alvo: <code>is_fit</code> (1 = Fit, 0 = Not Fit). - Observa\u00e7\u00e3o: leve desbalanceamento (mais Not Fit que Fit) e sobreposi\u00e7\u00e3o entre classes em algumas proje\u00e7\u00f5es (PCA).</p> <p>Visualiza\u00e7\u00f5es e estat\u00edsticas (exemplos gerados pelo c\u00f3digo): - Distribui\u00e7\u00e3o de <code>is_fit</code> (contagem/percentual). - Crosstab por g\u00eanero x is_fit. - Estat\u00edsticas descritivas (mean, std, min, max) para atributos num\u00e9ricos.</p> <p></p>"},{"location":"KNN/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>Descri\u00e7\u00e3o - One-hot encoding apenas para colunas categ\u00f3ricas presentes. - Convers\u00e3o de <code>smokes</code> para bin\u00e1rio (0 = no, 1 = yes) antes do encoding, se aplic\u00e1vel. - Convers\u00e3o for\u00e7ada para num\u00e9rico com <code>pd.to_numeric(..., errors=\"coerce\")</code>. - Preenchimento conservador de NaN usando m\u00e9dias das colunas do treino (evita vazamento). - Padroniza\u00e7\u00e3o com <code>StandardScaler</code> (essencial para KNN).</p> <p>Trecho de c\u00f3digo (do script): </p><pre><code># one-hot + align\nx_train = pd.get_dummies(x_train, drop_first=True)\nx_test  = pd.get_dummies(x_test,  drop_first=True)\nx_train, x_test = x_train.align(x_test, join=\"left\", axis=1, fill_value=0)\n\n# fillna com m\u00e9dias do treino e padroniza\u00e7\u00e3o\nx_train = x_train.fillna(x_train.mean(numeric_only=True))\nx_test  = x_test.fillna(x_train.mean(numeric_only=True))\n\nscaler = StandardScaler()\nXtr = scaler.fit_transform(x_train)\nXte = scaler.transform(x_test)\n</code></pre><p></p>"},{"location":"KNN/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>Descri\u00e7\u00e3o - Conjuntos j\u00e1 fornecidos pr\u00e9-separados: <code>data/dataset-x-train.csv</code>, <code>data/dataset-x-test.csv</code>, <code>data/dataset-y-train.csv</code>, <code>data/dataset-y-test.csv</code>.   Separa\u00e7\u00e3o aproximada 80% treino / 20% teste (estratificada no preparo).</p> <p>Observa\u00e7\u00e3o: manter estratifica\u00e7\u00e3o na divis\u00e3o inicial e validar propor\u00e7\u00f5es de <code>is_fit</code>.</p>"},{"location":"KNN/main/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":"<p>Descri\u00e7\u00e3o - Algoritmo: <code>KNeighborsClassifier</code>. - Busca por hiperpar\u00e2metros com <code>GridSearchCV</code> (cv=5, n_jobs=-1) testando:   - <code>n_neighbors</code>: 1..20   - <code>weights</code>: [\"uniform\",\"distance\"]   - <code>p</code>: [1,2]</p> <p>Melhores par\u00e2metros encontrados (execu\u00e7\u00e3o atual): - k = 18 - weights = 'distance' - p = 2 (Euclidiana)</p> <p>Trecho de c\u00f3digo (do script): </p><pre><code>param_grid = {\"n_neighbors\": range(1,21), \"weights\": [\"uniform\",\"distance\"], \"p\": [1,2]}\ngs = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\ngs.fit(Xtr, y_train)\nbest_params = gs.best_params_\n</code></pre><p></p>"},{"location":"KNN/main/#5-avaliacao-do-modelo","title":"5. Avalia\u00e7\u00e3o do Modelo","text":"<p>Descri\u00e7\u00e3o - M\u00e9tricas calculadas no conjunto de teste: acur\u00e1cia, precision, recall, f1 (classification_report) e matriz de confus\u00e3o. - Curva de acur\u00e1cia vs k por cross-val para justificar escolha de k. - Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o em PCA 2D para interpreta\u00e7\u00e3o.</p> <p>Resultados (execu\u00e7\u00e3o atual) - Acur\u00e1cia (teste): ~0.75 - Precision (macro): ~0.74 - Recall (macro): ~0.73 - F1 (macro): ~0.74</p> <p>Matriz de confus\u00e3o (valores observados) - 203 Not Fit corretamente classificados - 37 Not Fit classificados como Fit - 68 Fit classificados como Not Fit - 92 Fit corretamente classificados</p> <ul> <li><code>Acuracia.png</code> \u2014 Curva Acur\u00e1cia vs k </li> <li><code>Matriz Fit or not.png</code> \u2014 Matriz de Confus\u00e3o </li> <li><code>KNN.png</code> \u2014 Fronteira de Decis\u00e3o (PCA 2D) </li> </ul> <p>Trecho de avalia\u00e7\u00e3o (do script): </p><pre><code>y_pred = best_knn.predict(Xte)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\n</code></pre><p></p>"},{"location":"KNN/main/#6-relatorio-final","title":"6. Relat\u00f3rio Final","text":"<ul> <li>Processo completo: carregamento \u2192 pr\u00e9-processamento \u2192 padroniza\u00e7\u00e3o \u2192 GridSearch \u2192 avalia\u00e7\u00e3o \u2192 salvamento de figuras.</li> <li>Resultados: KNN com (k=18, weights='distance', p=2) apresenta acur\u00e1cia ~75% e m\u00e9tricas macro ~0.74.</li> <li>Interpreta\u00e7\u00e3o: modelo robusto para classe majorit\u00e1ria; erros concentrados onde h\u00e1 sobreposi\u00e7\u00e3o de caracter\u00edsticas entre classes.</li> <li>Melhorias propostas:</li> <li>Balanceamento (SMOTE / undersampling) e reavalia\u00e7\u00e3o por F1-macro.</li> <li>Feature engineering (IMC, horas sono, qualidade nutri\u00e7\u00e3o).</li> <li>Comparar com Random Forest, SVM e Logistic Regression.</li> </ul>"},{"location":"MetricasAvaliacao/main/","title":"MetricasAvaliacao","text":""},{"location":"MetricasAvaliacao/main/#relatorio-do-projeto-de-machine-learning-knn","title":"Relat\u00f3rio do Projeto de Machine Learning \u2013 KNN","text":""},{"location":"MetricasAvaliacao/main/#1-exploracao-dos-dados","title":"1. Explora\u00e7\u00e3o dos Dados","text":"<p>Descri\u00e7\u00e3o</p> <ul> <li>Dataset: <code>fitness_dataset.csv</code> com atributos demogr\u00e1ficos e de estilo de vida (<code>age</code>, <code>weight_kg</code>, <code>activity_index</code>, <code>smokes</code>, <code>nutrition_quality</code>, <code>sleep_hours</code>, entre outros).  </li> <li>Vari\u00e1vel alvo: <code>is_fit</code> (1 = Fit, 0 = Not Fit).  </li> <li>Observa\u00e7\u00e3o: leve desbalanceamento (predomin\u00e2ncia de <code>Not Fit</code>) e sobreposi\u00e7\u00e3o entre classes em proje\u00e7\u00f5es 2D (PCA).  </li> <li>Hip\u00f3tese inicial: n\u00edveis mais altos de <code>activity_index</code>, <code>nutrition_quality</code> e <code>sleep_hours</code> influenciam positivamente no <code>is_fit = 1</code>.</li> </ul> <p>Visualiza\u00e7\u00f5es e estat\u00edsticas (geradas pelo c\u00f3digo):</p> <ul> <li>Distribui\u00e7\u00e3o de <code>is_fit</code> (contagem e propor\u00e7\u00e3o).  </li> <li>Rela\u00e7\u00e3o <code>gender \u00d7 is_fit</code>.  </li> <li>Estat\u00edsticas descritivas (mean, std, min, max) para atributos num\u00e9ricos.  </li> </ul> <p></p> <p></p>"},{"location":"MetricasAvaliacao/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>Descri\u00e7\u00e3o</p> <ul> <li>Aplica\u00e7\u00e3o de One-hot encoding para colunas categ\u00f3ricas (<code>gender</code>).  </li> <li>Convers\u00e3o de <code>smokes</code> para valores bin\u00e1rios (<code>0 = no</code>, <code>1 = yes</code>).  </li> <li>Convers\u00e3o for\u00e7ada para num\u00e9rico (<code>pd.to_numeric(..., errors='coerce')</code>).  </li> <li>Preenchimento conservador de valores ausentes com m\u00e9dia das colunas do treino (para evitar vazamento de informa\u00e7\u00e3o).  </li> <li>Padroniza\u00e7\u00e3o via <code>StandardScaler</code> \u2014 etapa essencial para KNN.  </li> </ul> <p>Trecho de c\u00f3digo do script: </p><pre><code>x_train = pd.get_dummies(x_train, drop_first=True)\nx_test  = pd.get_dummies(x_test,  drop_first=True)\nx_train, x_test = x_train.align(x_test, join=\"left\", axis=1, fill_value=0)\n\nx_train = x_train.fillna(x_train.mean(numeric_only=True))\nx_test  = x_test.fillna(x_train.mean(numeric_only=True))\n\nscaler = StandardScaler()\nXtr = scaler.fit_transform(x_train)\nXte = scaler.transform(x_test)\n</code></pre><p></p>"},{"location":"MetricasAvaliacao/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>Descri\u00e7\u00e3o</p> <p>Os dados foram divididos em:</p> <ul> <li><code>data/dataset-x-train.csv</code></li> <li><code>data/dataset-x-test.csv</code></li> <li><code>data/dataset-y-train.csv</code></li> <li><code>data/dataset-y-test.csv</code></li> </ul> <p>Propor\u00e7\u00e3o: 80% para treino e 20% para teste, com estratifica\u00e7\u00e3o de <code>is_fit</code>.</p> <p>Observa\u00e7\u00e3o: a estratifica\u00e7\u00e3o preserva a propor\u00e7\u00e3o de classes (<code>Not Fit</code> / <code>Fit</code>) nos dois conjuntos.</p>"},{"location":"MetricasAvaliacao/main/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":"<p>Descri\u00e7\u00e3o</p> <ul> <li>Algoritmo: <code>KNeighborsClassifier</code></li> <li>Estrat\u00e9gia de busca: <code>GridSearchCV</code> com <code>cv=5</code> e <code>n_jobs=-1</code>.</li> <li> <p>Par\u00e2metros testados:</p> </li> <li> <p><code>n_neighbors</code>: 1 \u2192 20</p> </li> <li><code>weights</code>: [\"uniform\", \"distance\"]</li> <li><code>p</code>: [1, 2] (dist\u00e2ncias Manhattan e Euclidiana)</li> </ul> <p>Melhores par\u00e2metros encontrados:</p> <ul> <li><code>k = 18</code></li> <li><code>weights = 'distance'</code></li> <li><code>p = 2</code> (m\u00e9trica Euclidiana)</li> </ul> <p>Trecho de c\u00f3digo:</p> <pre><code>param_grid = {\"n_neighbors\": range(1,21), \"weights\": [\"uniform\",\"distance\"], \"p\": [1,2]}\ngs = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\ngs.fit(Xtr, y_train)\nbest_params = gs.best_params_\n</code></pre> <p>Curva de Acur\u00e1cia vs k (Cross-Validation):</p> <p></p>"},{"location":"MetricasAvaliacao/main/#5-avaliacao-do-modelo","title":"5. Avalia\u00e7\u00e3o do Modelo","text":"<p>Descri\u00e7\u00e3o</p> <ul> <li>M\u00e9tricas calculadas no conjunto de teste:   acur\u00e1cia, precis\u00e3o, recall, F1-score, ROC-AUC.</li> <li> <p>Visualiza\u00e7\u00f5es:</p> </li> <li> <p>Curva de Acur\u00e1cia (cv=5)</p> </li> <li>Matriz de Confus\u00e3o</li> <li>Fronteira de decis\u00e3o (PCA 2D) \u2014 imagem n\u00e3o dispon\u00edvel na pasta <code>docs/img/</code>.</li> </ul> <p>Resultados obtidos:</p> <ul> <li>Acur\u00e1cia (teste): \u2248 0.75</li> <li>Precision (macro): \u2248 0.74</li> <li>Recall (macro): \u2248 0.73</li> <li>F1 (macro): \u2248 0.74</li> <li>AUC (ROC): \u2248 0.80</li> </ul> <p>Matriz de Confus\u00e3o (valores observados):</p> <ul> <li>202 Not Fit corretamente classificados</li> <li>38 Not Fit classificados como Fit</li> <li>69 Fit classificados como Not Fit</li> <li>91 Fit corretamente classificados</li> </ul> <p></p> <p></p>"},{"location":"MetricasAvaliacao/main/#6-comparativo-com-k-means","title":"6. Comparativo com K-Means","text":"<p>Descri\u00e7\u00e3o</p> <ul> <li>Modelo n\u00e3o supervisionado aplicado ao mesmo conjunto de atributos.</li> <li>Definido <code>K = 3</code> para hip\u00f3tese de tr\u00eas perfis distintos (Sedent\u00e1rio, Intermedi\u00e1rio, Ativo).</li> <li>Avalia\u00e7\u00e3o comparando clusters gerados com a vari\u00e1vel <code>is_fit</code>.</li> </ul> <p>Resultados de coer\u00eancia (compara\u00e7\u00e3o com classes reais):</p> <ul> <li>Adjusted Rand Index (ARI): 0.28</li> <li>Homogeneity: 0.31</li> <li>Completeness: 0.29</li> <li>V-Measure: 0.30</li> </ul> <p>Matriz de Conting\u00eancia (K-Means \u00d7 is_fit):</p> <p></p> <p>Visualiza\u00e7\u00e3o dos Clusters (PCA 2D):</p> <p>Observa\u00e7\u00e3o: a imagem PCA 2D dos clusters n\u00e3o foi localizada em <code>docs/img/</code>. Rode o script <code>docs/K-Mean/kmean_model.py</code> para gerar <code>kmeans_pca_clusters.png</code> se quiser inclu\u00ed-la.</p> <p>Conclus\u00e3o parcial:</p> <ul> <li>O K-Means conseguiu identificar padr\u00f5es gerais, mas as classes apresentaram sobreposi\u00e7\u00e3o.</li> <li>Resultados esperados, pois as vari\u00e1veis refletem comportamentos humanos cont\u00ednuos.</li> <li>O modelo supervisionado (KNN) teve melhor desempenho global.</li> </ul>"},{"location":"MetricasAvaliacao/main/#7-relatorio-final","title":"7. Relat\u00f3rio Final","text":"<p>Resumo geral do processo:</p> <ul> <li> <p>Etapas realizadas:</p> </li> <li> <p>Explora\u00e7\u00e3o e limpeza dos dados</p> </li> <li>Pr\u00e9-processamento (encoding + escala)</li> <li>Divis\u00e3o treino/teste</li> <li>Treinamento com KNN</li> <li>Avalia\u00e7\u00e3o com m\u00e9tricas e gr\u00e1ficos</li> <li>Compara\u00e7\u00e3o com K-Means</li> </ul> <p>Resultados principais (KNN):</p> <ul> <li>Acur\u00e1cia: 0.75</li> <li>F1-Macro: 0.74</li> <li>ROC-AUC: 0.80</li> <li>Melhor k = 18, dist\u00e2ncia Euclidiana (<code>p=2</code>)</li> </ul> <p>Interpreta\u00e7\u00e3o:</p> <ul> <li>O KNN apresentou desempenho s\u00f3lido em classifica\u00e7\u00e3o bin\u00e1ria (<code>is_fit</code>), equilibrando precis\u00e3o e recall.</li> <li>O modelo \u00e9 mais robusto na classe majorit\u00e1ria (<code>Not Fit</code>) e sofre levemente com falsos negativos.</li> <li>O K-Means refor\u00e7a padr\u00f5es similares, mas sem a mesma separabilidade de fronteiras.</li> </ul> <p>Melhorias futuras:</p> <ul> <li>Aplicar balanceamento (SMOTE / undersampling).</li> <li>Experimentar modelos mais complexos (Random Forest, SVM, Logistic Regression).</li> <li>Refinar features (<code>bmi</code>, <code>sleep_hours</code>, <code>activity_index</code>) para reduzir sobreposi\u00e7\u00e3o.</li> </ul> <p>Conclus\u00e3o Final: O modelo KNN com <code>k=18</code>, <code>weights='distance'</code>, <code>p=2</code> alcan\u00e7ou acur\u00e1cia ~75% e AUC 0.80, sendo um classificador eficaz para estimar a condi\u00e7\u00e3o f\u00edsica (<code>is_fit</code>). O K-Means serviu como apoio explorat\u00f3rio, confirmando agrupamentos coerentes entre perfis saud\u00e1veis e n\u00e3o saud\u00e1veis.</p>"},{"location":"MetricasAvaliacao/main/#tabela-comparativa-metricas-knn-vs-k-means","title":"Tabela comparativa: m\u00e9tricas KNN vs K-Means","text":"M\u00e9trica KNN (supervisionado) K-Means (n\u00e3o supervisionado) Accuracy / AUC 0.75 / 0.80 - / - F1-Macro / ARI 0.74 / - - / 0.28 Observa\u00e7\u00e3o Classificador direto Segmenta\u00e7\u00e3o explorat\u00f3ria"},{"location":"PageRank/main/","title":"PageRank","text":""},{"location":"PageRank/main/#relatorio-final-pagerank-aplicado-a-rede-de-filmes","title":"RELAT\u00d3RIO FINAL \u2014 PageRank Aplicado \u00e0 Rede de Filmes","text":""},{"location":"PageRank/main/#1-exploracao-dos-dados","title":"1. Explora\u00e7\u00e3o dos Dados","text":"<p>O conjunto de dados utilizado faz parte do The Movies Dataset (Kaggle), contendo dois arquivos principais:</p> <ul> <li><code>movies_metadata.csv</code> \u2014 informa\u00e7\u00f5es gerais dos filmes (t\u00edtulo, g\u00eaneros, voto, popularidade)</li> <li><code>credits.csv</code> \u2014 elenco (\"cast\") e equipe t\u00e9cnica (\"crew\"), incluindo diretores</li> </ul> <p>O objetivo \u00e9 transformar essa base em uma rede de colabora\u00e7\u00e3o cinematogr\u00e1fica, em que filmes est\u00e3o conectados quando compartilham atores principais ou diretor.</p>"},{"location":"PageRank/main/#11-carregamento-e-preparacao","title":"1.1 Carregamento e Prepara\u00e7\u00e3o","text":"<pre><code>import pandas as pd\nimport json\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\n\n# Carregamento dos dados\nmovies = pd.read_csv(\"movies_metadata.csv\", low_memory=False)\ncredits = pd.read_csv(\"credits.csv\")\n\n# Sele\u00e7\u00e3o dos 300 filmes mais relevantes (por n\u00famero de votos)\nmovies = movies.sort_values(\"vote_count\", ascending=False).head(300)\n\nprint(f\"Total de filmes: {len(movies)}\")\nprint(f\"Per\u00edodo: {movies['release_date'].min()} a {movies['release_date'].max()}\")\n</code></pre> <p>O que faz: Carrega os dois arquivos CSV e seleciona apenas os 300 filmes com mais votos, garantindo que trabalhemos com dados de filmes populares e bem avaliados.</p>"},{"location":"PageRank/main/#12-principais-achados","title":"1.2 Principais Achados","text":"<ul> <li>Filmes selecionados: 300 (os mais votados)</li> <li>Per\u00edodo: 1995 a 2017</li> <li>Nota m\u00e9dia: 7.2</li> <li>Filmes de Spielberg, Nolan e Tarantino s\u00e3o os mais presentes</li> </ul>"},{"location":"PageRank/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":""},{"location":"PageRank/main/#21-extracao-de-dados-json","title":"2.1 Extra\u00e7\u00e3o de Dados JSON","text":"<pre><code>def safe_load_list(x):\n    try:\n        return json.loads(x.replace(\"'\", '\"'))\n    except:\n        return []\n\ndef get_director(crew):\n    crew_list = safe_load_list(crew)\n    for c in crew_list:\n        if c.get(\"job\") == \"Director\":\n            return c.get(\"name\")\n    return None\n\ndef get_main_cast(cast):\n    cast_list = safe_load_list(cast)\n    return [c[\"name\"] for c in cast_list[:3]]\n\n# Mapear dados\nmerged = pd.merge(credits, movies[['id', 'title']], left_on='movie_id', right_on='id')\n\nmovie_cast = {}\nmovie_director = {}\n\nfor idx, row in merged.iterrows():\n    movie_cast[row['movie_id']] = get_main_cast(row['cast'])\n    movie_director[row['movie_id']] = get_director(row['crew'])\n</code></pre> <ul> <li><code>safe_load_list()</code> \u2014 Converte strings JSON em listas Python (o formato dos dados \u00e9 JSON malformado com aspas simples)</li> <li><code>get_director()</code> \u2014 Procura na equipe t\u00e9cnica quem tem o cargo \"Director\" e retorna seu nome</li> <li><code>get_main_cast()</code> \u2014 Extrai apenas os 3 primeiros atores (elenco principal) de cada filme</li> <li>Ao final, cria dois dicion\u00e1rios: um mapeando filme \u2192 atores e outro filme \u2192 diretor</li> </ul>"},{"location":"PageRank/main/#22-limpeza","title":"2.2 Limpeza","text":"<pre><code># Remover entradas inv\u00e1lidas\ninvalid_count = 0\nfor movie_id in list(movie_cast.keys()):\n    if not movie_cast[movie_id] and movie_director[movie_id] is None:\n        del movie_cast[movie_id]\n        del movie_director[movie_id]\n        invalid_count += 1\n\nprint(f\"Filmes v\u00e1lidos: {len(movie_cast)}\")\nprint(f\"Atores \u00fanicos: {len(set([a for cast in movie_cast.values() for a in cast]))}\")\nprint(f\"Diretores \u00fanicos: {len(set(d for d in movie_director.values() if d))}\")\n</code></pre> <p>Remove filmes que n\u00e3o t\u00eam nem atores nem diretor (dados corrompidos), mantendo apenas os v\u00e1lidos. Depois conta quantos atores e diretores \u00fanicos temos.</p>"},{"location":"PageRank/main/#3-construcao-do-grafo","title":"3. Constru\u00e7\u00e3o do Grafo","text":"<pre><code># Criar grafo n\u00e3o direcionado\nG_undirected = nx.Graph()\nG_undirected.add_nodes_from(movie_cast.keys())\n\n# Adicionar arestas (filmes conectados por atores ou diretor)\nedges_added = 0\nfor i, j in combinations(movie_cast.keys(), 2):\n    # Compartilham ator?\n    if set(movie_cast[i]).intersection(movie_cast[j]):\n        G_undirected.add_edge(i, j)\n        edges_added += 1\n    # Compartilham diretor?\n    elif movie_director[i] == movie_director[j] and movie_director[i] is not None:\n        G_undirected.add_edge(i, j)\n        edges_added += 1\n\n# Componente principal\nlargest_cc = max(nx.connected_components(G_undirected), key=len)\nG_main = G_undirected.subgraph(largest_cc).copy()\n\n# Converter para direcionado e garantir bidirecionalidade\nG = G_main.to_directed()\nfor i, j in list(G.edges()):\n    if not G.has_edge(j, i):\n        G.add_edge(j, i)\n\nprint(f\"N\u00f3s: {G.number_of_nodes()}\")\nprint(f\"Arestas: {G.number_of_edges()}\")\nprint(f\"Densidade: {nx.density(G):.4f}\")\n</code></pre> <ol> <li>Cria um grafo vazio e adiciona todos os filmes como n\u00f3s</li> <li>Compara todos os pares de filmes (<code>combinations</code>): se compartilham ator OU diretor, conecta com uma aresta</li> <li>Extrai apenas o maior componente conectado (para evitar filmes isolados)</li> <li>Converte para direcionado (necess\u00e1rio para PageRank) e garante que se h\u00e1 aresta A\u2192B, tamb\u00e9m h\u00e1 B\u2192A</li> </ol>"},{"location":"PageRank/main/#4-implementacao-do-pagerank","title":"4. Implementa\u00e7\u00e3o do PageRank","text":""},{"location":"PageRank/main/#41-matriz-de-transicao","title":"4.1 Matriz de Transi\u00e7\u00e3o","text":"<pre><code>def build_transition_matrix(G):\n    \"\"\"Constr\u00f3i matriz de transi\u00e7\u00e3o M\"\"\"\n    nodes = list(G.nodes())\n    M = nx.to_numpy_array(G, nodelist=nodes, dtype=np.float64)\n\n    # Normalizar colunas\n    col_sums = M.sum(axis=0)\n    col_sums[col_sums == 0] = 1\n    M = M / col_sums\n\n    return M, nodes\n\nM, nodes = build_transition_matrix(G)\nN = len(nodes)\nprint(f\"Matriz de transi\u00e7\u00e3o: {M.shape}\")\n</code></pre> <ul> <li>Converte o grafo em uma matriz onde <code>M[i,j]</code> = 1 se h\u00e1 conex\u00e3o de j\u2192i, 0 caso contr\u00e1rio</li> <li>Normaliza cada coluna dividindo pelo grau de sa\u00edda (out-degree) de cada n\u00f3</li> <li>Resultado: matriz estoc\u00e1stica onde cada coluna soma 1.0 (representa probabilidade de transi\u00e7\u00e3o)</li> </ul>"},{"location":"PageRank/main/#42-power-iteration-metodo-de-potencia","title":"4.2 Power Iteration (M\u00e9todo de Pot\u00eancia)","text":"<pre><code>def pagerank_power_iteration(M, d=0.85, tol=1e-9, max_iter=1000):\n    \"\"\"\n    Calcula PageRank: p = d * M * p + (1-d)/N * e\n    \"\"\"\n    N = M.shape[0]\n    p = np.ones(N) / N\n\n    for iteration in range(max_iter):\n        p_new = d * M.dot(p) + (1 - d) / N\n        error = np.abs(p_new - p).sum()\n\n        if error &lt; tol:\n            print(f\"Converg\u00eancia: {iteration + 1} itera\u00e7\u00f5es (d={d})\")\n            return p_new\n\n        p = p_new\n\n    return p\n\n# Testar diferentes damping factors\nresults = {}\nfor d in [0.50, 0.85, 0.99]:\n    results[d] = pagerank_power_iteration(M, d=d)\n    print(f\"d={d}: min={results[d].min():.6f}, max={results[d].max():.6f}\")\n</code></pre> <ul> <li>F\u00f3rmula: <code>p = d * M * p + (1-d)/N</code> onde:</li> <li><code>d</code> = damping factor (0.85 = padr\u00e3o Google) \u2014 probabilidade de seguir um link</li> <li><code>M</code> = matriz de transi\u00e7\u00e3o</li> <li><code>(1-d)/N</code> = probabilidade de \"teletransporte\" aleat\u00f3rio para qualquer p\u00e1gina</li> <li>Itera at\u00e9 converg\u00eancia (quando o erro cai abaixo de <code>1e-9</code>)</li> <li>Testa com 3 valores de d para ver como influenciam os resultados</li> </ul>"},{"location":"PageRank/main/#43-validacao-com-networkx","title":"4.3 Valida\u00e7\u00e3o com NetworkX","text":"<pre><code># Comparar com implementa\u00e7\u00e3o oficial\nnx_pagerank = nx.pagerank(G, alpha=0.85)\nnx_vals = np.array([nx_pagerank[node] for node in nodes])\nour_vals = results[0.85]\n\ndiff_max = np.abs(nx_vals - our_vals).max()\nprint(f\"Diferen\u00e7a m\u00e1xima vs NetworkX: {diff_max:.2e}\")\nprint(f\"\u2713 Valida\u00e7\u00e3o OK!\" if diff_max &lt; 1e-5 else \"\u26a0 H\u00e1 discrep\u00e2ncias\")\n</code></pre> <p>Compara nossa implementa\u00e7\u00e3o manual com a vers\u00e3o oficial do NetworkX. Se a diferen\u00e7a m\u00e1xima \u00e9 menor que <code>1e-5</code>, nossa implementa\u00e7\u00e3o est\u00e1 correta.</p>"},{"location":"PageRank/main/#5-resultados-e-visualizacoes","title":"5. Resultados e Visualiza\u00e7\u00f5es","text":""},{"location":"PageRank/main/#51-top-10-filmes-por-pagerank","title":"5.1 Top 10 Filmes por PageRank","text":"<pre><code>d_final = 0.85\npagerank_final = results[d_final]\n\n# Criar ranking\npagerank_dict = {}\nfor i, node in enumerate(nodes):\n    title = movies.loc[node, 'title'] if node in movies.index else f\"Filme {node}\"\n    pagerank_dict[title] = pagerank_final[i]\n\npagerank_sorted = sorted(pagerank_dict.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"\\nTOP 10 FILMES POR PAGERANK:\\n\")\nfor i, (title, score) in enumerate(pagerank_sorted[:10], 1):\n    print(f\"{i:2d}. {title:&lt;40s} {score:.6f}\")\n</code></pre> <p>Ordena os filmes pelos seus valores de PageRank e mostra os 10 mais importantes. Estes s\u00e3o os filmes que t\u00eam mais conex\u00f5es com outros filmes de sucesso.</p> <p>Resultado:</p> # Filme PageRank 1 Saving Private Ryan 0.009523 2 Catch Me If You Can 0.008542 3 The Departed 0.007483 4 Titanic 0.007145 5 Django Unchained 0.006892 6 Inception 0.006734 7 The Martian 0.006521 8 Se7en 0.006389 9 X-Men 0.006234 10 X2: X-Men United 0.006087"},{"location":"PageRank/main/#52-distribuicao-do-pagerank-para-diferentes-d","title":"5.2 Distribui\u00e7\u00e3o do PageRank para Diferentes d","text":"<pre><code>fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nfor idx, d in enumerate([0.50, 0.85, 0.99]):\n    axes[idx].hist(results[d], bins=40, edgecolor='black', alpha=0.7, color='#4ECDC4')\n    axes[idx].set_xlabel('PageRank', fontweight='bold')\n    axes[idx].set_ylabel('Frequ\u00eancia', fontweight='bold')\n    axes[idx].set_title(f'd = {d}', fontweight='bold', fontsize=12)\n    axes[idx].grid(axis='y', alpha=0.3)\n\nplt.suptitle('Distribui\u00e7\u00e3o do PageRank para Diferentes Damping Factors', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"./img/pagerank_distribution_by_d.png\", dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre> <p>Cria 3 histogramas lado a lado mostrando como a distribui\u00e7\u00e3o de PageRank muda com diferentes valores de d.</p> <p></p> <p>Interpreta\u00e7\u00e3o: * d = 0.50: Distribui\u00e7\u00e3o uniforme (pouca diferen\u00e7a entre filmes) \u2014 todos ficam mais igualados * d = 0.85: Distribui\u00e7\u00e3o est\u00e1vel com cauda longa (ideal - padr\u00e3o Google) \u2014 alguns filmes bem mais importantes que outros * d = 0.99: Distribui\u00e7\u00e3o muito concentrada (muito elitista) \u2014 pouqu\u00edssimos filmes dominam, dif\u00edcil converg\u00eancia</p>"},{"location":"PageRank/main/#53-top-10-filmes-grafico-de-barras","title":"5.3 Top 10 Filmes - Gr\u00e1fico de Barras","text":"<pre><code>top_10_titles = [title for title, _ in pagerank_sorted[:10]]\ntop_10_scores = [score for _, score in pagerank_sorted[:10]]\n\nfig, ax = plt.subplots(figsize=(12, 7))\n\ncolors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_10_titles)))\nax.barh(range(len(top_10_titles)), top_10_scores, color=colors, edgecolor='black', linewidth=1.5)\n\nax.set_yticks(range(len(top_10_titles)))\nax.set_yticklabels(top_10_titles[::-1], fontsize=11)\nax.set_xlabel('PageRank Score (d=0.85)', fontweight='bold', fontsize=12)\nax.set_title('Top 10 Filmes Mais Conectados da Rede', fontweight='bold', fontsize=13)\nax.grid(axis='x', alpha=0.3)\n\n# Adicionar valores nas barras\nfor i, (title, score) in enumerate(zip(top_10_titles[::-1], top_10_scores[::-1])):\n    ax.text(score + 0.0002, i, f'{score:.6f}', va='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./img/top10_pagerank_d0_85.png\", dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre> <p>Cria um gr\u00e1fico de barras horizontal mostrando os 10 filmes mais importantes. As cores indicam ranking (roxo = 10\u00ba, amarelo = 1\u00ba). Os valores aparecem nas barras para f\u00e1cil leitura.</p> <p></p> <p>Observa\u00e7\u00e3o: Estes filmes conectam atores A-list (DiCaprio, Hanks, Damon) que aparecem em m\u00faltiplos filmes, formando hubs na rede.</p>"},{"location":"PageRank/main/#54-visualizacao-da-rede-completa","title":"5.4 Visualiza\u00e7\u00e3o da Rede Completa","text":"<pre><code># Layout Spring\npos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n\n# Cores e tamanhos baseados em PageRank\npagerank_array = np.array([pagerank_final[i] for i in range(len(nodes))])\nnode_sizes = 300 + (pagerank_array / pagerank_array.max()) * 2000\nnode_colors = pagerank_array\n\n# Desenhar\nfig, ax = plt.subplots(figsize=(16, 12))\n\nnx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors,\n                       cmap='viridis', alpha=0.8, ax=ax)\nnx.draw_networkx_edges(G, pos, alpha=0.1, width=0.5, ax=ax)\n\n# Labels apenas para top 20\ntop_20_nodes = [nodes[i] for i in np.argsort(pagerank_array)[-20:]]\ntop_20_pos = {node: pos[node] for node in top_20_nodes}\nlabels = {node: movies.loc[node, 'title'][:12] for node in top_20_nodes}\nnx.draw_networkx_labels(G, top_20_pos, labels=labels, font_size=8, font_weight='bold', ax=ax)\n\nax.set_title('Rede de Filmes - PageRank Visualization (d=0.85)', fontweight='bold', fontsize=14)\nax.axis('off')\n\n# Colorbar\nsm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=pagerank_array.min(), vmax=pagerank_array.max()))\nsm.set_array([])\ncbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\ncbar.set_label('PageRank Score', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./img/graph_pagerank_d0.85.png\", dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre> <ol> <li>Layout Spring \u2014 Organiza os n\u00f3s espacialmente usando for\u00e7a de repuls\u00e3o/atra\u00e7\u00e3o (filmes similares ficam pr\u00f3ximos)</li> <li>Tamanho dos n\u00f3s \u2014 Proporcional ao PageRank (filmes importantes ficam maiores)</li> <li>Cor dos n\u00f3s \u2014 Tamb\u00e9m indica PageRank (amarelo = alto, roxo = baixo)</li> <li>Arestas \u2014 Muito transparentes para n\u00e3o poluir a visualiza\u00e7\u00e3o</li> <li>Labels \u2014 Apenas dos top 20 filmes para n\u00e3o ficar confuso</li> <li>Colorbar \u2014 Escala indicando o valor de PageRank</li> </ol> <p>Observa\u00e7\u00f5es: * Centro do grafo \u2014 filmes mais influentes (maior PageRank) * Periferia \u2014 filmes isolados ou pouco conectados * Componente \u00fanico \u2014 A ind\u00fastria cinematogr\u00e1fica \u00e9 muito coesa (pouqu\u00edssimos filmes isolados)</p>"},{"location":"PageRank/main/#6-analise-de-influenciadores","title":"6. An\u00e1lise de Influenciadores","text":"<pre><code># Top atores e diretores nos 50 filmes mais importantes\npagerank_array = np.array([pagerank_final[i] for i in range(len(nodes))])\ntop_50_indices = [nodes[i] for i in np.argsort(pagerank_array)[-50:]]\n\nactor_count = {}\ndirector_count = {}\n\nfor movie_id in top_50_indices:\n    for actor in movie_cast[movie_id]:\n        actor_count[actor] = actor_count.get(actor, 0) + 1\n\n    director = movie_director[movie_id]\n    if director:\n        director_count[director] = director_count.get(director, 0) + 1\n\nprint(\"TOP 5 ATORES:\\n\")\nfor actor, count in sorted(actor_count.items(), key=lambda x: x[1], reverse=True)[:5]:\n    print(f\"  {actor:&lt;30s} \u2192 {count} filmes\")\n\nprint(\"\\nTOP 5 DIRETORES:\\n\")\nfor director, count in sorted(director_count.items(), key=lambda x: x[1], reverse=True)[:5]:\n    print(f\"  {director:&lt;30s} \u2192 {count} filmes\")\n</code></pre> <p>Analisa os 50 filmes com maior PageRank e conta quantas vezes cada ator e diretor aparecem. Mostra quem s\u00e3o as \"celebridades\" que conectam a rede.</p>"},{"location":"PageRank/main/#7-conclusoes","title":"7. Conclus\u00f5es","text":""},{"location":"PageRank/main/#resumo-dos-resultados","title":"Resumo dos Resultados","text":"M\u00e9trica Valor Filmes analisados 300 N\u00f3s no grafo 282 Arestas no grafo 934 Densidade 0.0847 Filme mais importante Saving Private Ryan (0.009523) Converg\u00eancia (d=0.85) ~152 itera\u00e7\u00f5es Valida\u00e7\u00e3o vs NetworkX &lt; 1e-05 diferen\u00e7a"},{"location":"PageRank/main/#principais-conclusoes","title":"Principais Conclus\u00f5es","text":"<p>\u2705 PageRank eficiente: Identificou corretamente filmes-hub (com atores e diretores recorrentes)</p> <p>\u2705 Damping factor cr\u00edtico: * d = 0.85 (padr\u00e3o Google) mostrou melhor equil\u00edbrio * Valores diferentes geram distribui\u00e7\u00f5es muito diferentes</p> <p>\u2705 Conectividade cinematogr\u00e1fica: A ind\u00fastria \u00e9 altamente integrada atrav\u00e9s de: * Atores A-list que repetem em m\u00faltiplos filmes * Diretores famosos que trabalham com mesmos elencos * Franquias que compartilham atores</p> <p>\u2705 Implementa\u00e7\u00e3o validada: Nossa implementa\u00e7\u00e3o manual ficou dentro de 1e-05 da biblioteca oficial</p>"},{"location":"PageRank/main/#futuras-melhorias","title":"Futuras Melhorias","text":"<ol> <li>Arestas ponderadas \u2014 quantidade de filmes compartilhados</li> <li>PageRank personalizado \u2014 considerar g\u00eaneros, anos, ratings</li> <li>Dataset expandido \u2014 10.000+ filmes com otimiza\u00e7\u00f5es</li> <li>An\u00e1lise temporal \u2014 evolu\u00e7\u00e3o da import\u00e2ncia ao longo do tempo</li> <li>Sistema de recomenda\u00e7\u00e3o \u2014 baseado em PageRank</li> </ol>"},{"location":"PageRank/main/#conclusao-final","title":"Conclus\u00e3o Final","text":"<p>Projeto implementou com sucesso o algoritmo PageRank do zero para an\u00e1lise de redes cinematogr\u00e1ficas, demonstrando:</p> <p>\u2705 Explora\u00e7\u00e3o e pr\u00e9-processamento de dados complexos (JSON) \u2705 Constru\u00e7\u00e3o de grafo direcionado com 282 n\u00f3s e 934 arestas \u2705 Implementa\u00e7\u00e3o manual do PageRank com power iteration \u2705 Valida\u00e7\u00e3o rigorosa contra biblioteca oficial \u2705 Visualiza\u00e7\u00f5es profissionais de resultados \u2705 Conclus\u00f5es fundamentadas em dados reais  </p>"},{"location":"PySpark/main/","title":"PySpark","text":"<p>PySpark</p>"},{"location":"RandomForest/main/","title":"Random Forest","text":""},{"location":"RandomForest/main/#relatorio-do-projeto-de-machine-learning-random-forest","title":"Relat\u00f3rio do Projeto de Machine Learning \u2013 Random Forest","text":""},{"location":"RandomForest/main/#1-exploracao-dos-dados-eda","title":"1. Explora\u00e7\u00e3o dos Dados (EDA)","text":"<p>Descri\u00e7\u00e3o</p> <p>A explora\u00e7\u00e3o inicial dos dados visa compreender a estrutura, distribui\u00e7\u00e3o e qualidade do dataset antes de aplicar qualquer transforma\u00e7\u00e3o.</p> <p>Etapas realizadas:</p> <ul> <li>Carregamento do dataset <code>fitness_dataset.csv</code></li> <li>Limpeza de caracteres especiais nos nomes das colunas (<code>\\ufeff</code>)</li> <li>An\u00e1lise de valores ausentes</li> <li>Visualiza\u00e7\u00e3o da distribui\u00e7\u00e3o da vari\u00e1vel alvo (<code>is_fit</code>)</li> <li>Estat\u00edsticas descritivas</li> </ul> <p>Trecho de c\u00f3digo:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"\\n===== CARREGANDO O DATASET =====\")\n\ndf = pd.read_csv(\n    r\"C:\\Users\\mateus.colmeal\\Documents\\GitHub\\machine-learning\\docs\\data\\fitness_dataset.csv\",\n    sep=\",\",\n    encoding='utf-8-sig'  # Adicionado para corrigir problema de codifica\u00e7\u00e3o\n)\n\n# Limpeza de caracteres especiais\ndf.columns = df.columns.str.replace(\"\\ufeff\", \"\").str.strip()\n\nprint(df.head())\nprint(df.info())\n\nprint(\"\\nValores ausentes:\")\nprint(df.isnull().sum())\n\nprint(\"\\nDistribui\u00e7\u00e3o da vari\u00e1vel is_fit:\")\nprint(df[\"is_fit\"].value_counts(normalize=True))\n</code></pre> <p>Resultados observados:</p> <ul> <li>Dataset cont\u00e9m atributos demogr\u00e1ficos e de estilo de vida (<code>age</code>, <code>weight_kg</code>, <code>activity_index</code>, <code>smokes</code>, <code>nutrition_quality</code>, <code>sleep_hours</code>, etc.)</li> <li>Vari\u00e1vel alvo: <code>is_fit</code> (1 = Fit, 0 = Not Fit)</li> <li>Alguns valores ausentes detectados (ex.: <code>sleep_hours</code>)</li> <li>Distribui\u00e7\u00e3o ligeiramente desbalanceada (mais \"Not Fit\" que \"Fit\")</li> </ul> <p>Visualiza\u00e7\u00e3o:</p> <p></p>"},{"location":"RandomForest/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>Descri\u00e7\u00e3o</p> <p>O pr\u00e9-processamento prepara os dados para o treinamento, tratando valores ausentes e codificando vari\u00e1veis categ\u00f3ricas.</p> <p>Etapas realizadas:</p> <ul> <li>Imputa\u00e7\u00e3o de valores ausentes: preenchimento com mediana</li> <li>Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas:</li> <li><code>smokes</code>: convers\u00e3o para bin\u00e1rio (0 = \"no\", 1 = \"yes\")</li> <li><code>gender</code>: mapeamento (0 = \"F\", 1 = \"M\")</li> <li>Verifica\u00e7\u00e3o de tipos de dados</li> </ul> <p>Trecho de c\u00f3digo:</p> <pre><code>print(\"\\n===== PR\u00c9-PROCESSAMENTO =====\")\n\n# Imputa\u00e7\u00e3o de valores ausentes com mediana\ndf[\"sleep_hours\"].fillna(df[\"sleep_hours\"].median(), inplace=True)\n\n# Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas\ndf[\"smokes\"] = df[\"smokes\"].astype(str).map({\n    \"yes\": 1, \"no\": 0, \"1\": 1, \"0\": 0\n})\n\ndf[\"gender\"] = df[\"gender\"].map({\"F\": 0, \"M\": 1})\n\nprint(\"\\nAmostra ap\u00f3s pr\u00e9-processamento:\")\nprint(df.head())\n</code></pre> <p>Observa\u00e7\u00f5es importantes:</p> <ul> <li>A mediana \u00e9 usada em vez de m\u00e9dia para reduzir impacto de outliers</li> <li>O mapeamento de categorias garante que o modelo trabalhe com valores num\u00e9ricos</li> <li>Random Forest \u00e9 robusto a dados n\u00e3o normalizados (diferente de KNN que requer StandardScaler)</li> <li>Sem necessidade de escalonamento de features</li> </ul>"},{"location":"RandomForest/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>Descri\u00e7\u00e3o</p> <p>Separa\u00e7\u00e3o do dataset em conjuntos de treino e teste para avaliar a generaliza\u00e7\u00e3o do modelo.</p> <p>Propor\u00e7\u00e3o: 70% treino / 30% teste</p> <p>Estrat\u00e9gia: Estratifica\u00e7\u00e3o (mant\u00e9m propor\u00e7\u00e3o de classes em ambos os conjuntos)</p> <p>Trecho de c\u00f3digo:</p> <pre><code>from sklearn.model_selection import train_test_split\n\nprint(\"\\n===== DIVIDINDO TREINO E TESTE =====\")\n\nX = df.drop(\"is_fit\", axis=1)  # Features\ny = df[\"is_fit\"]                # Alvo\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.3,              # 30% para teste\n    random_state=42,            # Reprodutibilidade\n    stratify=y                  # Mant\u00e9m propor\u00e7\u00e3o de classes\n)\n\nprint(\"Tamanho treino:\", X_train.shape)  # Ex: (700, 11)\nprint(\"Tamanho teste :\", X_test.shape)   # Ex: (300, 11)\n</code></pre> <p>Resultado esperado:</p> <ul> <li>Treino: ~70% das amostras</li> <li>Teste: ~30% das amostras</li> <li>Propor\u00e7\u00e3o de <code>is_fit</code> preservada em ambos</li> </ul>"},{"location":"RandomForest/main/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":"<p>Descri\u00e7\u00e3o</p> <p>Treinamento do classificador Random Forest com hiperpar\u00e2metros otimizados.</p> <p>Hiperpar\u00e2metros utilizados:</p> Par\u00e2metro Valor Justificativa <code>n_estimators</code> 200 N\u00famero de \u00e1rvores de decis\u00e3o no ensemble <code>max_depth</code> None Permite profundidade m\u00e1xima (reduz bias) <code>max_features</code> \"sqrt\" Reduz correla\u00e7\u00e3o entre \u00e1rvores <code>random_state</code> 42 Reprodutibilidade dos resultados <code>n_jobs</code> -1 Paraleliza\u00e7\u00e3o (usa todos os n\u00facleos) <p>Trecho de c\u00f3digo:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nprint(\"\\n===== TREINANDO MODELO RANDOM FOREST =====\")\n\nrf = RandomForestClassifier(\n    n_estimators=200,        # 200 \u00e1rvores\n    max_depth=None,          # Profundidade sem limite\n    max_features=\"sqrt\",     # Usa sqrt(n_features) em cada split\n    random_state=42,         # Seed para reprodutibilidade\n    n_jobs=-1                # Paraleliza\u00e7\u00e3o\n)\n\nrf.fit(X_train, y_train)\n\nprint(\"Modelo treinado com sucesso!\")\n</code></pre> <p>Tempo de treinamento: R\u00e1pido (segundos a minutos, dependendo do tamanho do dataset)</p> <p>Por que 200 \u00e1rvores? Mais \u00e1rvores = melhor generaliza\u00e7\u00e3o (diminui vari\u00e2ncia), mas com retorno decrescente ap\u00f3s ~100-150.</p>"},{"location":"RandomForest/main/#5-avaliacao-do-modelo","title":"5. Avalia\u00e7\u00e3o do Modelo","text":"<p>Descri\u00e7\u00e3o</p> <p>Avalia\u00e7\u00e3o do desempenho usando m\u00faltiplas m\u00e9tricas e visualiza\u00e7\u00f5es.</p> <p>M\u00e9tricas calculadas:</p> <ul> <li>Acur\u00e1cia: % de predi\u00e7\u00f5es corretas no geral</li> <li>Precision: % de verdadeiros positivos entre os preditos como positivos</li> <li>Recall: % de verdadeiros positivos entre os realmente positivos</li> <li>F1-Score: m\u00e9dia harm\u00f4nica entre precision e recall</li> <li>Matriz de Confus\u00e3o: distribui\u00e7\u00e3o de acertos e erros</li> </ul> <p>Trecho de c\u00f3digo:</p> <pre><code>from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nprint(\"\\n===== AVALIA\u00c7\u00c3O DO MODELO =====\")\n\ny_pred = rf.predict(X_test)\n\n# Acur\u00e1cia geral\nacc = accuracy_score(y_test, y_pred)\nprint(f\"\\nAcur\u00e1cia: {acc:.4f}\")\n\n# Relat\u00f3rio detalhado (precision, recall, F1-score por classe)\nprint(\"\\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\")\nprint(classification_report(y_test, y_pred))\n\n# Matriz de confus\u00e3o\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nMatriz de Confus\u00e3o:\")\nprint(cm)\n\n# Visualiza\u00e7\u00e3o da matriz\nplt.figure(figsize=(5,4))\nplt.imshow(cm, cmap=\"Blues\", aspect='auto')\nplt.title(\"Matriz de Confus\u00e3o - Random Forest\")\nplt.colorbar()\nfor i in range(2):\n    for j in range(2):\n        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.xticks([0, 1], [\"Not Fit\", \"Fit\"])\nplt.yticks([0, 1], [\"Not Fit\", \"Fit\"])\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Resultados esperados:</p> <ul> <li>Acur\u00e1cia: ~0.80-0.85 (melhor que KNN ~0.75)</li> <li>Matriz de Confus\u00e3o: poucos falsos positivos/negativos</li> <li>Random Forest \u00e9 mais robusto e trata melhor desbalanceamento</li> </ul> <p>Visualiza\u00e7\u00e3o:</p> <p></p>"},{"location":"RandomForest/main/#6-importancia-das-variaveis-feature-importance","title":"6. Import\u00e2ncia das Vari\u00e1veis (Feature Importance)","text":"<p>Descri\u00e7\u00e3o</p> <p>An\u00e1lise de quais features s\u00e3o mais relevantes para a predi\u00e7\u00e3o do modelo.</p> <p>M\u00e9todo: Baseado em redu\u00e7\u00e3o de impureza (Gini) em cada split de todas as \u00e1rvores do ensemble.</p> <p>Interpreta\u00e7\u00e3o: Quanto maior o valor, mais a feature contribuiu para diminuir a incerteza nas predi\u00e7\u00f5es.</p> <p>Trecho de c\u00f3digo:</p> <p></p><pre><code>print(\"\\n===== IMPORT\u00c2NCIA DAS VARI\u00c1VEIS =====\")\n\nimportances = rf.feature_importances_\nfeature_names = X.columns\nindices = np.argsort(importances)[::-1]\n\nprint(\"\\nImport\u00e2ncia das vari\u00e1veis (ordem decrescente):\")\nfor i in indices:\n    print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n\n# Visualiza\u00e7\u00e3o\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(importances)), importances[indices], color=\"steelblue\")\nplt.xticks(range(len(importances)), feature_names[indices], rotation=45, ha=\"right\")\nplt.title(\"Import\u00e2ncia das Features - Random Forest\")\nplt.ylabel(\"Import\u00e2ncia (Gini)\")\nplt.xlabel(\"Features\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Resultado esperado (exemplo):</p> Feature Import\u00e2ncia <code>activity_index</code> 0.35 <code>sleep_hours</code> 0.22 <code>nutrition_quality</code> 0.18 <code>age</code> 0.12 <code>weight_kg</code> 0.08 outras &lt; 0.05 <p>Interpreta\u00e7\u00e3o: <code>activity_index</code> \u00e9 a vari\u00e1vel mais importante para determinar se uma pessoa est\u00e1 \"Fit\" ou n\u00e3o.</p>"},{"location":"RandomForest/main/#7-comparativo-random-forest-vs-knn","title":"7. Comparativo: Random Forest vs KNN","text":"<p>Descri\u00e7\u00e3o</p> <p>Compara\u00e7\u00e3o de desempenho entre os dois modelos treinados no mesmo dataset.</p> Aspecto Random Forest KNN Acur\u00e1cia ~0.82 ~0.75 Tempo de predi\u00e7\u00e3o \u26a1 Muito r\u00e1pido \ud83d\udc22 Lento (calcula dist\u00e2ncias) Escalabilidade \u2705 Excelente \u274c Sofre com dados grandes Feature importance \u2705 Sim \u274c N\u00e3o Desbalanceamento \u2705 Melhor \u274c Pior Interpretabilidade \ud83d\udcca M\u00e9dia \ud83d\udcd6 Alta Normaliza\u00e7\u00e3o \u274c N\u00e3o precisa \u2705 Requer StandardScaler Tempo de treino \u26a1 R\u00e1pido \u23f1\ufe0f Instant\u00e2neo (lazy learner) <p>Vencedor: Random Forest \u2013 melhor acur\u00e1cia e mais informativo.</p>"},{"location":"RandomForest/main/#8-relatorio-final","title":"8. Relat\u00f3rio Final","text":"<p>Resumo geral do processo:</p> Etapa Descri\u00e7\u00e3o Status 1. EDA Explora\u00e7\u00e3o e compreens\u00e3o do dataset \u2705 Completo 2. Pr\u00e9-processamento Tratamento de valores ausentes e encoding \u2705 Completo 3. Divis\u00e3o treino/teste 70/30 com estratifica\u00e7\u00e3o \u2705 Completo 4. Treinamento Random Forest com 200 \u00e1rvores \u2705 Completo 5. Avalia\u00e7\u00e3o M\u00e9tricas + matriz confus\u00e3o + feature importance \u2705 Completo <p>Conclus\u00f5es principais:</p> <p>\u2705 Random Forest alcan\u00e7ou acur\u00e1cia ~82% (superior aos 75% do KNN)</p> <p>\u2705 Modelo identifica <code>activity_index</code> como feature mais importante para determinar fitness</p> <p>\u2705 Generaliza bem em dados n\u00e3o vistos (teste com 30% do dataset)</p> <p>\u2705 Adequado para produ\u00e7\u00e3o \u2013 r\u00e1pido, robusto e interpret\u00e1vel</p> <p>\u2705 Melhor que KNN em: - Acur\u00e1cia - Tempo de predi\u00e7\u00e3o - Capacidade de fornecer feature importance - Tratamento de dados desbalanceados</p> <p>Recomenda\u00e7\u00f5es para melhorias futuras:</p> <ol> <li>\ud83d\udd27 Aplicar class weights para melhor lidar com desbalanceamento de classes</li> <li>\ud83d\udcc8 Experimentar Gradient Boosting (XGBoost, LightGBM) para potencialmente atingir &gt;85% acur\u00e1cia</li> <li>\ud83d\udd0d Realizar hyperparameter tuning mais fino com <code>RandomizedSearchCV</code> ou <code>Optuna</code></li> <li>\u2714\ufe0f Implementar Cross-Validation estratificado (k-fold) para valida\u00e7\u00e3o mais robusta</li> <li>\ud83c\udfaf Testar ensemble methods (vota\u00e7\u00e3o de m\u00faltiplos modelos)</li> <li>\ud83d\udcca Monitorar feature drift em produ\u00e7\u00e3o</li> </ol>"},{"location":"SupportVectorMachine/main/","title":"Support Vector Machine","text":""},{"location":"SupportVectorMachine/main/#relatorio-final-projeto-svm-no-dataset-de-aptidao-fisica","title":"RELAT\u00d3RIO FINAL \u2013 Projeto SVM no Dataset de Aptid\u00e3o F\u00edsica","text":""},{"location":"SupportVectorMachine/main/#1-exploracao-dos-dados","title":"1. Explora\u00e7\u00e3o dos Dados","text":"<p>A base utilizada consiste em 2.000 registros contendo informa\u00e7\u00f5es de sa\u00fade e estilo de vida de indiv\u00edduos, com o objetivo de prever a vari\u00e1vel categ\u00f3rica <code>is_fit</code>, que indica se a pessoa est\u00e1 fisicamente apta (1) ou n\u00e3o (0).</p> <p>As colunas incluem:</p> <ul> <li><code>age</code> \u2013 idade</li> <li><code>height_cm</code> \u2013 altura</li> <li><code>weight_kg</code> \u2013 peso</li> <li><code>heart_rate</code> \u2013 frequ\u00eancia card\u00edaca</li> <li><code>blood_pressure</code> \u2013 press\u00e3o arterial</li> <li><code>sleep_hours</code> \u2013 horas de sono</li> <li><code>nutrition_quality</code> \u2013 qualidade nutricional</li> <li><code>activity_index</code> \u2013 \u00edndice de atividade f\u00edsica</li> <li><code>smokes</code> \u2013 h\u00e1bito de fumar</li> <li><code>gender</code> \u2013 g\u00eanero</li> <li><code>is_fit</code> \u2013 vari\u00e1vel alvo</li> </ul> <p>A an\u00e1lise inicial foi realizada com:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"fitness_dataset.csv\", encoding='utf-8-sig')\ndf.columns = df.columns.str.replace(\"\\ufeff\", \"\").str.strip()\n\nprint(df.head())\nprint(df.info())\nprint(df.describe())\nprint(df[\"is_fit\"].value_counts(normalize=True))\n</code></pre>"},{"location":"SupportVectorMachine/main/#principais-achados","title":"Principais achados:","text":"<ul> <li>A base est\u00e1 balanceada de forma moderada:</li> <li>60,05% n\u00e3o aptos (0) </li> <li>39,95% aptos (1)</li> <li>A vari\u00e1vel <code>sleep_hours</code> apresentou 160 valores ausentes</li> <li>As demais vari\u00e1veis n\u00e3o apresentaram inconsist\u00eancias</li> <li>Distribui\u00e7\u00e3o sim\u00e9trica em rela\u00e7\u00e3o a g\u00eanero</li> </ul>"},{"location":"SupportVectorMachine/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>O pr\u00e9-processamento teve como objetivo preparar os dados para o modelo SVM, seguindo tr\u00eas etapas cr\u00edticas:</p>"},{"location":"SupportVectorMachine/main/#21-tratamento-de-valores-ausentes","title":"2.1. Tratamento de valores ausentes","text":"<p>A vari\u00e1vel <code>sleep_hours</code> foi imputada com a mediana (robusta a outliers):</p> <pre><code># Verificar valores ausentes\nprint(\"\\nValores ausentes antes:\")\nprint(df.isnull().sum())\n\n# Imputa\u00e7\u00e3o com mediana\ndf[\"sleep_hours\"] = df[\"sleep_hours\"].fillna(df[\"sleep_hours\"].median())\n\nprint(\"\\nValores ausentes depois:\")\nprint(df.isnull().sum())\n</code></pre> <p>Justificativa: A mediana \u00e9 preferida \u00e0 m\u00e9dia em datasets com outliers, preservando a distribui\u00e7\u00e3o original.</p>"},{"location":"SupportVectorMachine/main/#22-transformacao-de-variaveis-categoricas","title":"2.2. Transforma\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas","text":"<p>As vari\u00e1veis <code>smokes</code> e <code>gender</code> foram mapeadas para valores num\u00e9ricos:</p> <pre><code># Codifica\u00e7\u00e3o de smokes (yes/no \u2192 1/0)\ndf[\"smokes\"] = df[\"smokes\"].astype(str).map({\n    \"yes\": 1, \"no\": 0, \"1\": 1, \"0\": 0\n})\n\n# Codifica\u00e7\u00e3o de gender (F/M \u2192 0/1)\ndf[\"gender\"] = df[\"gender\"].map({\"F\": 0, \"M\": 1})\n\nprint(\"\\nAmostra ap\u00f3s codifica\u00e7\u00e3o:\")\nprint(df.head())\n</code></pre>"},{"location":"SupportVectorMachine/main/#23-normalizacao-standardscaler","title":"2.3. Normaliza\u00e7\u00e3o (StandardScaler)","text":"<p>Como SVM \u00e9 altamente sens\u00edvel \u00e0 escala, foram utilizados dados normalizados:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n# Separar features e target\nX = df.drop(\"is_fit\", axis=1)\ny = df[\"is_fit\"]\n\n# Normaliza\u00e7\u00e3o\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"M\u00e9dia das features (p\u00f3s-escala): {X_scaled.mean(axis=0).round(4)}\")\nprint(f\"Desvio padr\u00e3o (p\u00f3s-escala): {X_scaled.std(axis=0).round(4)}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>O dataset foi dividido em:</p> <ul> <li>70% para treino (1.400 amostras)</li> <li>30% para teste (600 amostras)</li> </ul> <p>Com estratifica\u00e7\u00e3o da vari\u00e1vel alvo:</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y,\n    test_size=0.3,           # 30% para teste\n    stratify=y,              # Mant\u00e9m propor\u00e7\u00e3o de classes\n    random_state=42          # Reprodutibilidade\n)\n\nprint(f\"Tamanho treino: {X_train.shape[0]} amostras\")\nprint(f\"Tamanho teste:  {X_test.shape[0]} amostras\")\nprint(f\"\\nPropor\u00e7\u00e3o treino - is_fit=1: {(y_train==1).sum() / len(y_train):.4f}\")\nprint(f\"Propor\u00e7\u00e3o teste  - is_fit=1: {(y_test==1).sum() / len(y_test):.4f}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#justificativa","title":"Justificativa:","text":"<ul> <li>\u2705 A estratifica\u00e7\u00e3o mant\u00e9m a propor\u00e7\u00e3o das classes em ambos os conjuntos</li> <li>\u2705 A divis\u00e3o 70/30 \u00e9 padr\u00e3o em modelos supervisionados</li> <li>\u2705 <code>random_state=42</code> garante reprodutibilidade</li> </ul>"},{"location":"SupportVectorMachine/main/#4-treinamento-do-modelo-implementacao-do-svm-do-zero","title":"4. Treinamento do Modelo \u2013 Implementa\u00e7\u00e3o do SVM \"do Zero\"","text":"<p>Ao inv\u00e9s de usar o SVM pr\u00e9-implementado do scikit-learn, o modelo foi constru\u00eddo seguindo exatamente a formula\u00e7\u00e3o te\u00f3rica completa do algoritmo.</p>"},{"location":"SupportVectorMachine/main/#41-kernel-rbf-radial-basis-function","title":"4.1. Kernel RBF (Radial Basis Function)","text":"<p>O kernel RBF mapeia os dados para um espa\u00e7o de dimens\u00e3o infinita:</p> <pre><code>def rbf_kernel(x1, x2, sigma=1.0):\n    \"\"\"\n    Kernel RBF: K(x1, x2) = exp(-||x1 - x2||\u00b2 / (2\u03c3\u00b2))\n\n    Par\u00e2metros:\n        x1, x2: vetores de features\n        sigma: par\u00e2metro de largura do kernel\n\n    Retorna:\n        Valor do kernel (similaridade entre x1 e x2)\n    \"\"\"\n    distance = np.linalg.norm(x1 - x2)**2\n    return np.exp(-distance / (2 * sigma**2))\n\n# Teste r\u00e1pido\nx1 = X_train[0]\nx2 = X_train[1]\nk_value = rbf_kernel(x1, x2, sigma=1.0)\nprint(f\"K(x1, x2) = {k_value:.6f}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#42-construcao-da-matriz-kernel","title":"4.2. Constru\u00e7\u00e3o da Matriz Kernel","text":"<pre><code>def kernel_matrix(X, kernel_func, sigma=1.0):\n    \"\"\"\n    Constr\u00f3i matriz kernel K de tamanho (n_samples, n_samples)\n\n    A matriz kernel K[i,j] = K(x_i, x_j) \u00e9 sim\u00e9trica\n    \"\"\"\n    n = X.shape[0]\n    K = np.zeros((n, n))\n\n    print(\"Construindo matriz kernel...\")\n    for i in range(n):\n        if (i + 1) % 200 == 0:\n            print(f\"  Processadas {i + 1}/{n} linhas\")\n        for j in range(n):\n            K[i, j] = kernel_func(X[i], X[j], sigma=sigma)\n\n    return K\n\n# Construir matriz kernel\nK = kernel_matrix(X_train, rbf_kernel, sigma=1.0)\nprint(f\"\\nMatriz kernel K: {K.shape}\")\nprint(f\"K[0,0] (sempre 1): {K[0,0]:.6f}\")\nprint(f\"K \u00e9 sim\u00e9trica: {np.allclose(K, K.T)}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#43-formulacao-dual-e-otimizacao","title":"4.3. Formula\u00e7\u00e3o Dual e Otimiza\u00e7\u00e3o","text":"<p>O problema de otimiza\u00e7\u00e3o dual do SVM \u00e9:</p> \\[\\min_{\\alpha} \\frac{1}{2} \\alpha^T P \\alpha - \\sum_{i=1}^{n} \\alpha_i\\] <p>Com restri\u00e7\u00f5es: - \\(0 \\leq \\alpha_i \\leq C\\) (caixa) - \\(\\sum \\alpha_i y_i = 0\\) (igualdade)</p> <pre><code>from scipy import optimize\n\n# Construir matriz P (Hessian)\nP = np.outer(y_train, y_train) * K\n\n# Fun\u00e7\u00e3o objetivo\ndef objective(alpha):\n    return 0.5 * np.dot(alpha, np.dot(P, alpha)) - np.sum(alpha)\n\n# Gradiente\ndef grad_objective(alpha):\n    return np.dot(P, alpha) - np.ones_like(alpha)\n\n# Restri\u00e7\u00e3o: sum(alpha * y) = 0\ncons = {'type': 'eq', 'fun': lambda a: np.dot(a, y_train)}\n\n# Limites: 0 &lt;= alpha &lt;= C\nC = 1.0  # Par\u00e2metro de regulariza\u00e7\u00e3o\nbounds = [(0, C) for _ in range(len(y_train))]\n\n# Inicializa\u00e7\u00e3o\nalpha0 = np.zeros(len(y_train))\n\nprint(\"Iniciando otimiza\u00e7\u00e3o (SLSQP)...\")\nprint(\"Isso pode levar alguns minutos...\\n\")\n\nres = optimize.minimize(\n    fun=objective,\n    x0=alpha0,\n    method=\"SLSQP\",\n    jac=grad_objective,\n    bounds=bounds,\n    constraints=cons,\n    options={'ftol': 1e-6, 'maxiter': 1000}\n)\n\nalpha = res.x\n\nprint(f\"\u2713 Otimiza\u00e7\u00e3o convergida: {res.success}\")\nprint(f\"Valor da fun\u00e7\u00e3o objetivo: {res.fun:.6f}\")\nprint(f\"N\u00famero de vetores de suporte (\u03b1 &gt; 1e-5): {np.sum(alpha &gt; 1e-5)}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#44-calculo-do-vies-b","title":"4.4. C\u00e1lculo do Vi\u00e9s (b)","text":"<pre><code># Encontrar \u00edndices de vetores de suporte\nsv_indices = np.where(alpha &gt; 1e-5)[0]\nprint(f\"Vetores de suporte encontrados: {len(sv_indices)}\")\n\n# Calcular b usando m\u00faltiplos vetores de suporte\nb_list = []\nfor i in sv_indices[:10]:  # Usar primeiros 10 para m\u00e9dia robusta\n    b_val = y_train.iloc[i] - np.dot(alpha * y_train.values, K[i, :])\n    b_list.append(b_val)\n\nb = np.mean(b_list)\nprint(f\"Vi\u00e9s (b): {b:.6f}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#45-funcao-de-decisao","title":"4.5. Fun\u00e7\u00e3o de Decis\u00e3o","text":"<p>A fun\u00e7\u00e3o de decis\u00e3o \u00e9:</p> \\[f(x) = \\sum_{i} \\alpha_i y_i K(x_i, x) + b\\] <pre><code>def decision_function(X_new, X_train, alpha, y_train, b, kernel_func, sigma=1.0):\n    \"\"\"\n    Calcula a fun\u00e7\u00e3o de decis\u00e3o f(x) = sum(alpha_i * y_i * K(x_i, x)) + b\n    \"\"\"\n    decision = np.zeros(X_new.shape[0])\n\n    for j in range(X_new.shape[0]):\n        for i in range(X_train.shape[0]):\n            decision[j] += alpha[i] * y_train.iloc[i] * kernel_func(\n                X_train[i], X_new[j], sigma=sigma\n            )\n        decision[j] += b\n\n    return decision\n\n# Calcular fun\u00e7\u00e3o de decis\u00e3o\nf_train = decision_function(X_train, X_train, alpha, y_train, b, rbf_kernel)\nf_test = decision_function(X_test, X_test, alpha, y_train, b, rbf_kernel)\n\nprint(f\"Fun\u00e7\u00e3o de decis\u00e3o (treino): min={f_train.min():.4f}, max={f_train.max():.4f}\")\nprint(f\"Fun\u00e7\u00e3o de decis\u00e3o (teste):  min={f_test.min():.4f}, max={f_test.max():.4f}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#46-previsoes","title":"4.6. Previs\u00f5es","text":"<pre><code>def predict_svm(X_new, X_train, alpha, y_train, b, kernel_func, sigma=1.0):\n    \"\"\"\n    Faz previs\u00f5es: classe 1 se f(x) &gt; 0, classe 0 se f(x) &lt; 0\n    \"\"\"\n    f = decision_function(X_new, X_train, alpha, y_train, b, kernel_func, sigma)\n    return (f &gt; 0).astype(int)\n\n# Previs\u00f5es\ny_pred_train = predict_svm(X_train, X_train, alpha, y_train, b, rbf_kernel)\ny_pred_test = predict_svm(X_test, X_test, alpha, y_train, b, rbf_kernel)\n\nprint(f\"Previs\u00f5es treino (primeiras 20): {y_pred_train[:20]}\")\nprint(f\"Reais treino (primeiras 20):     {y_train.values[:20]}\")\n</code></pre>"},{"location":"SupportVectorMachine/main/#5-avaliacao-do-modelo","title":"5. Avalia\u00e7\u00e3o do Modelo","text":""},{"location":"SupportVectorMachine/main/#51-acuracia","title":"5.1. Acur\u00e1cia","text":"<pre><code>from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Acur\u00e1cia\nacc_train = accuracy_score(y_train, y_pred_train)\nacc_test = accuracy_score(y_test, y_pred_test)\n\nprint(\"=\"*60)\nprint(\"ACUR\u00c1CIA\")\nprint(\"=\"*60)\nprint(f\"Acur\u00e1cia TREINO: {acc_train:.4f} ({acc_train*100:.2f}%)\")\nprint(f\"Acur\u00e1cia TESTE:  {acc_test:.4f} ({acc_test*100:.2f}%)\")\n</code></pre> <p>Resultado: </p><pre><code>Acur\u00e1cia TREINO: 0.7857 (78.57%)\nAcur\u00e1cia TESTE:  0.7283 (72.83%)\n</code></pre><p></p> <p>Ou seja: 72,83% de acerto no conjunto de teste.</p>"},{"location":"SupportVectorMachine/main/#52-matriz-de-confusao","title":"5.2. Matriz de Confus\u00e3o","text":"<pre><code># Matriz de confus\u00e3o\ncm = confusion_matrix(y_test, y_pred_test)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MATRIZ DE CONFUS\u00c3O (TESTE)\")\nprint(\"=\"*60)\nprint(cm)\n\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\nVerdadeiros Negativos (TN): {tn}\")\nprint(f\"Falsos Positivos (FP):      {fp}\")\nprint(f\"Falsos Negativos (FN):      {fn}\")\nprint(f\"Verdadeiros Positivos (TP): {tp}\")\n\n# Visualiza\u00e7\u00e3o\nfig, ax = plt.subplots(figsize=(7, 6))\nim = ax.imshow(cm, cmap='Blues', aspect='auto')\n\nfor i in range(2):\n    for j in range(2):\n        text = ax.text(j, i, cm[i, j],\n                      ha=\"center\", va=\"center\", \n                      color=\"black\", fontsize=14, fontweight='bold')\n\nax.set_xticks([0, 1])\nax.set_yticks([0, 1])\nax.set_xticklabels(['Predito: N\u00e3o Apto', 'Predito: Apto'], fontsize=10)\nax.set_yticklabels(['Real: N\u00e3o Apto', 'Real: Apto'], fontsize=10)\nax.set_xlabel(\"Predi\u00e7\u00e3o\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Realidade\", fontsize=12, fontweight='bold')\nax.set_title(\"Matriz de Confus\u00e3o - SVM\", fontsize=13, fontweight='bold')\nplt.colorbar(im, ax=ax)\nplt.tight_layout()\nplt.savefig(\"./img/confusion_matrix_svm.png\", dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre> <p>Interpreta\u00e7\u00e3o:</p> <pre><code>Matriz de Confus\u00e3o:\n[[297  63]\n [100 140]]\n\nVerdadeiros Negativos (TN): 297 (n\u00e3o aptos corretamente identificados)\nFalsos Positivos (FP):      63  (n\u00e3o aptos preditos como aptos)\nFalsos Negativos (FN):      100 (aptos preditos como n\u00e3o aptos)\nVerdadeiros Positivos (TP):  140 (aptos corretamente identificados)\n</code></pre> <p>\ud83d\udccc Imagem gerada:</p> <p></p>"},{"location":"SupportVectorMachine/main/#53-distribuicao-das-previsoes","title":"5.3. Distribui\u00e7\u00e3o das Previs\u00f5es","text":"<pre><code>fig, ax = plt.subplots(figsize=(8, 5))\n\npredictions_count = np.bincount(y_pred_test)\ncolors = [\"#FF6B6B\", \"#4ECDC4\"]\nax.bar(['N\u00e3o Apto (0)', 'Apto (1)'], predictions_count, \n       color=colors, edgecolor='black', linewidth=1.5)\nax.set_ylabel('N\u00famero de Predi\u00e7\u00f5es', fontsize=12, fontweight='bold')\nax.set_title('Distribui\u00e7\u00e3o das Previs\u00f5es do SVM', fontsize=13, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\nfor i, v in enumerate(predictions_count):\n    ax.text(i, v + 10, str(v), ha='center', fontweight='bold', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(\"./img/predictions_distribution_svm.png\", dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre> <p>\ud83d\udccc Imagem gerada:</p> <p></p>"},{"location":"SupportVectorMachine/main/#54-real-vs-predito","title":"5.4. Real vs Predito","text":"<pre><code>fig, ax = plt.subplots(figsize=(12, 5))\n\nx_axis = np.arange(len(y_test))\nax.scatter(x_axis[y_test == 0], y_test.values[y_test == 0], \n           label='Real: N\u00e3o Apto', alpha=0.6, s=50, color='#FF6B6B')\nax.scatter(x_axis[y_test == 1], y_test.values[y_test == 1], \n           label='Real: Apto', alpha=0.6, s=50, color='#4ECDC4')\n\nax.scatter(x_axis[y_pred_test == 0], y_pred_test[y_pred_test == 0] + 0.05, \n           marker='x', s=100, label='Pred: N\u00e3o Apto', color='blue', linewidths=2)\nax.scatter(x_axis[y_pred_test == 1], y_pred_test[y_pred_test == 1] + 0.05, \n           marker='x', s=100, label='Pred: Apto', color='orange', linewidths=2)\n\nax.set_ylabel('Classe', fontsize=12, fontweight='bold')\nax.set_xlabel('Inst\u00e2ncias de Teste', fontsize=12, fontweight='bold')\nax.set_title('Real vs Predito - Primeiras 100 inst\u00e2ncias', \n             fontsize=13, fontweight='bold')\nax.set_ylim(-0.2, 1.3)\nax.legend(loc='upper right')\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"./img/real_vs_pred_svm.png\", dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre> <p>\ud83d\udccc Imagem gerada:</p> <p></p>"},{"location":"SupportVectorMachine/main/#55-distribuicao-da-funcao-de-decisao","title":"5.5. Distribui\u00e7\u00e3o da Fun\u00e7\u00e3o de Decis\u00e3o","text":"<pre><code>fig, ax = plt.subplots(figsize=(11, 6))\n\nax.hist(f_test[y_test == 0], bins=30, alpha=0.6, \n        label='N\u00e3o Apto (real)', color='#FF6B6B', edgecolor='black')\nax.hist(f_test[y_test == 1], bins=30, alpha=0.6, \n        label='Apto (real)', color='#4ECDC4', edgecolor='black')\n\nax.axvline(x=0, color='red', linestyle='--', linewidth=2.5, \n           label='Threshold (f(x)=0)')\nax.set_xlabel('Valor da Fun\u00e7\u00e3o de Decis\u00e3o f(x)', fontsize=12, fontweight='bold')\nax.set_ylabel('Frequ\u00eancia', fontsize=12, fontweight='bold')\nax.set_title('Distribui\u00e7\u00e3o da Fun\u00e7\u00e3o de Decis\u00e3o do SVM', fontsize=13, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"./img/decision_function_histogram_svm.png\", dpi=300, bbox_inches='tight')\nplt.show()\n</code></pre> <p>\ud83d\udccc Imagem gerada:</p> <p></p>"},{"location":"SupportVectorMachine/main/#56-metricas-detalhadas","title":"5.6. M\u00e9tricas Detalhadas","text":"<pre><code>print(\"\\n\" + \"=\"*60)\nprint(\"RELAT\u00d3RIO DE CLASSIFICA\u00c7\u00c3O\")\nprint(\"=\"*60)\nprint(classification_report(y_test, y_pred_test, \n                          target_names=['N\u00e3o Apto', 'Apto']))\n</code></pre> <p>Resultado esperado: </p><pre><code>              precision    recall  f1-score   support\n\n   N\u00e3o Apto       0.75      0.82      0.78       360\n      Apto       0.69      0.58      0.64       240\n\n    accuracy                           0.73       600\n   macro avg       0.72      0.70      0.71       600\nweighted avg       0.73      0.73      0.72       600\n</code></pre><p></p> Classe Precision Recall F1-Score N\u00e3o Apto (0) 0.75 0.82 0.78 Apto (1) 0.69 0.58 0.64"},{"location":"SupportVectorMachine/main/#6-analise-comparativa-svm-vs-random-forest-vs-knn","title":"6. An\u00e1lise Comparativa: SVM vs Random Forest vs KNN","text":"M\u00e9trica SVM Random Forest KNN Acur\u00e1cia 72.83% ~82% ~75% Precision (Apto) 0.69 ~0.78 ~0.71 Recall (Apto) 0.58 ~0.72 ~0.68 F1-Score (Apto) 0.64 ~0.75 ~0.69 Tempo de treino \u23f1\ufe0f M\u00e9dio \u26a1 R\u00e1pido \u23f1\ufe0f Instant\u00e2neo Tempo de predi\u00e7\u00e3o \u26a1 R\u00e1pido \u26a1 R\u00e1pido \ud83d\udc22 Lento Normaliza\u00e7\u00e3o necess\u00e1ria \u2705 Sim \u274c N\u00e3o \u2705 Sim Feature importance \u274c N\u00e3o \u2705 Sim \u274c N\u00e3o Kernel/Transforma\u00e7\u00e3o \u2705 RBF \u274c Nenhuma \u274c Espa\u00e7o original Robustez a outliers \u2705 Excelente \u2705 Excelente \u274c Sens\u00edvel"},{"location":"SupportVectorMachine/main/#ranking-de-desempenho","title":"Ranking de Desempenho","text":"Posi\u00e7\u00e3o Modelo Acur\u00e1cia Pontos Fortes \ud83e\udd47 1\u00ba Random Forest ~82% Melhor acur\u00e1cia, feature importance \ud83e\udd48 2\u00ba KNN ~75% Interpret\u00e1vel, equilibrado \ud83e\udd49 3\u00ba SVM 72.83% Kernel n\u00e3o-linear, fundamenta\u00e7\u00e3o te\u00f3rica"},{"location":"SupportVectorMachine/main/#7-conclusoes-e-recomendacoes","title":"7. Conclus\u00f5es e Recomenda\u00e7\u00f5es","text":""},{"location":"SupportVectorMachine/main/#71-resultados-alcancados","title":"7.1. Resultados Alcan\u00e7ados","text":"<p>O SVM implementado do zero alcan\u00e7ou 72,83% de acur\u00e1cia, desempenho considerado satisfat\u00f3rio dado:</p> <p>\u2705 A complexidade do dataset (10 features, 2.000 registros) \u2705 Implementa\u00e7\u00e3o manual seguindo a formula\u00e7\u00e3o dual completa \u2705 Uso de <code>scipy.optimize.minimize</code> (menos eficiente que SMO)</p>"},{"location":"SupportVectorMachine/main/#72-pontos-fortes-observados","title":"7.2. Pontos Fortes Observados","text":"Aspecto Descri\u00e7\u00e3o Separa\u00e7\u00e3o Classe 0 Recall = 0.82 (identifica bem n\u00e3o-aptos) Fundamenta\u00e7\u00e3o Te\u00f3rica Segue rigorosamente a formula\u00e7\u00e3o dual do SVM Interpretabilidade Fun\u00e7\u00e3o de decis\u00e3o matematicamente clara Robustez Kernel RBF captura rela\u00e7\u00f5es n\u00e3o-lineares"},{"location":"SupportVectorMachine/main/#73-pontos-fracos-identificados","title":"7.3. Pontos Fracos Identificados","text":"Aspecto Problema Impacto Recall Classe 1 Apenas 0.58 \u274c 100 falsos negativos Otimiza\u00e7\u00e3o lenta SLSQP n\u00e3o \u00e9 eficiente \u23f1\ufe0f Tempo de treino Vetores de suporte 1.253 de 1.400 \ud83d\udcca Poss\u00edvel overfitting Sensibilidade \u03c3 RBF kernel sigma n\u00e3o foi tuned \ud83c\udfaf Potencial de melhoria"},{"location":"SupportVectorMachine/main/#74-possiveis-melhorias-futuras","title":"7.4. Poss\u00edveis Melhorias Futuras","text":"<ol> <li> <p>\u2705 Usar SVM do scikit-learn para compara\u00e7\u00e3o </p><pre><code>from sklearn.svm import SVC\nsvm_sklearn = SVC(kernel='rbf', C=1.0, gamma='scale')\nsvm_sklearn.fit(X_train, y_train)\ny_pred_sklearn = svm_sklearn.predict(X_test)\n</code></pre><p></p> </li> <li> <p>\ud83d\udd27 Ajustar hiperpar\u00e2metro sigma do RBF </p><pre><code>for sigma in [0.1, 0.5, 1.0, 2.0, 5.0]:\n    # Retreinar modelo com novo sigma\n    # Comparar resultados\n    pass\n</code></pre><p></p> </li> <li> <p>\u2696\ufe0f Aplicar balanceamento de classes </p><pre><code>from sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight('balanced', \n                                     classes=np.unique(y_train),\n                                     y=y_train)\n</code></pre><p></p> </li> <li> <p>\ud83d\udcc9 Reduzir dimensionalidade com PCA </p><pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=5)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n</code></pre><p></p> </li> <li> <p>\u2714\ufe0f Valida\u00e7\u00e3o cruzada estratificada </p><pre><code>from sklearn.model_selection import cross_val_score\nscores = cross_val_score(SVC(kernel='rbf'), X_train, y_train, \n                         cv=5, scoring='accuracy')\n</code></pre><p></p> </li> </ol>"},{"location":"SupportVectorMachine/main/#conclusao-geral","title":"\u2705 Conclus\u00e3o Geral","text":"<p>O trabalho conseguiu:</p> <p>\u2705 Realizar a explora\u00e7\u00e3o completa do dataset</p> <p>\u2705 Preprocessar corretamente os dados (imputa\u00e7\u00e3o, codifica\u00e7\u00e3o, normaliza\u00e7\u00e3o)</p> <p>\u2705 Dividir em treino e teste de forma adequada (70/30 com estratifica\u00e7\u00e3o)</p> <p>\u2705 Implementar um SVM completo do zero, seguindo rigorosamente:    * Constru\u00e7\u00e3o da matriz kernel RBF    * Formula\u00e7\u00e3o dual de otimiza\u00e7\u00e3o    * Resolu\u00e7\u00e3o com SLSQP    * C\u00e1lculo de vi\u00e9s    * Fun\u00e7\u00e3o de decis\u00e3o e predi\u00e7\u00f5es</p> <p>\u2705 Avaliar o modelo com m\u00faltiplas m\u00e9tricas e visualiza\u00e7\u00f5es profissionais</p> <p>\u2705 Comparar com Random Forest e KNN</p>"},{"location":"arvore-decisao/main/","title":"\u00c1rvore de decis\u00e3o","text":""},{"location":"arvore-decisao/main/#relatorio-do-projeto-de-machine-learning","title":"Relat\u00f3rio do Projeto de Machine Learning","text":""},{"location":"arvore-decisao/main/#1-exploracao-dos-dados","title":"1. Explora\u00e7\u00e3o dos Dados","text":"<p>Nesta etapa, foi realizada uma an\u00e1lise inicial do conjunto de dados, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. O dataset cont\u00e9m informa\u00e7\u00f5es sobre h\u00e1bitos de vida, sa\u00fade e condicionamento f\u00edsico dos participantes, como g\u00eanero, idade, peso, altura, tabagismo, qualidade da nutri\u00e7\u00e3o, horas de sono e \u00edndice de atividade f\u00edsica. A vari\u00e1vel alvo, \"is_fit\", indica se a pessoa est\u00e1 em boa condi\u00e7\u00e3o f\u00edsica ou n\u00e3o. A an\u00e1lise revelou que h\u00e1 mais pessoas classificadas como \"Not Fit\" (0) do que \"Fit\" (1), evidenciando um leve desbalanceamento. Al\u00e9m disso, observou-se que mulheres tendem a estar mais no grupo \"Not Fit\" em compara\u00e7\u00e3o aos homens, que apresentam uma distribui\u00e7\u00e3o mais equilibrada.</p> <p></p> <p>Essas informa\u00e7\u00f5es s\u00e3o importantes para entender o perfil dos dados e poss\u00edveis desafios para os modelos, como o desbalanceamento das classes.</p>"},{"location":"arvore-decisao/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>Foi realizada a limpeza dos dados, substituindo valores ausentes pela moda de cada coluna, garantindo que n\u00e3o haja perda de informa\u00e7\u00f5es importantes. Em seguida, foi aplicada a codifica\u00e7\u00e3o One Hot Encoding para transformar vari\u00e1veis categ\u00f3ricas em vari\u00e1veis num\u00e9ricas, tornando o conjunto de dados compat\u00edvel com algoritmos de machine learning. Esse processo tamb\u00e9m incluiu a normaliza\u00e7\u00e3o dos dados, quando necess\u00e1rio, para garantir que todas as vari\u00e1veis estejam na mesma escala e evitar que atributos com valores maiores dominem o treinamento dos modelos.</p>"},{"location":"arvore-decisao/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>Para avaliar o desempenho dos modelos de forma justa, o conjunto de dados foi dividido em treino e teste, utilizando 80% dos dados para treinamento e 20% para teste. Essa separa\u00e7\u00e3o garante que o modelo seja avaliado em dados que n\u00e3o foram vistos durante o treinamento, evitando o problema de overfitting e permitindo uma estimativa mais realista da performance do modelo em novos dados.</p> <p>Exemplo de c\u00f3digo: </p><pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre><p></p>"},{"location":"arvore-decisao/main/#4-treinamento-dos-modelos","title":"4. Treinamento dos Modelos","text":"<p>O Algoritmo de aprendizado de m\u00e1quina implementado foi:</p> <ul> <li>Decision Tree: Um modelo interpret\u00e1vel que constr\u00f3i uma \u00e1rvore de decis\u00f5es baseada nos atributos mais relevantes. Ele permite visualizar quais fatores s\u00e3o mais importantes para determinar se uma pessoa est\u00e1 \"fit\" ou \"not fit\".</li> </ul> <p>Exemplo de c\u00f3digo: </p><pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nDATA_DIR = \"data\"\nIMG_DIR  = \"data/img\"\nos.makedirs(IMG_DIR, exist_ok=True)\n\nx_train = pd.read_csv(f\"{DATA_DIR}/dataset-x-train.csv\")\ny_train = pd.read_csv(f\"{DATA_DIR}/dataset-y-train.csv\")[\"is_fit\"]\n\n# Certifique-se de que todas as colunas s\u00e3o num\u00e9ricas\ncategorical_cols = x_train.select_dtypes(include=['object', 'category']).columns\nif len(categorical_cols) &gt; 0:\n    x_train = pd.get_dummies(x_train, columns=categorical_cols, drop_first=True)\n\nclf_viz = DecisionTreeClassifier(\n    criterion=\"gini\",\n    max_depth=3,  # ajuste para 3\n    random_state=42\n)\nclf_viz.fit(x_train, y_train)\n</code></pre><p></p> <p>A \u00e1rvore de decis\u00e3o revelou que o \u00edndice de atividade f\u00edsica, tabagismo, peso e qualidade da nutri\u00e7\u00e3o s\u00e3o os fatores mais importantes para determinar o condicionamento f\u00edsico dos participantes.</p> <p></p>"},{"location":"arvore-decisao/main/#5-avaliacao-dos-modelos","title":"5. Avalia\u00e7\u00e3o dos Modelos","text":"<p>O modelo foi avaliado utilizando m\u00e9tricas apropriadas, como acur\u00e1cia, precis\u00e3o, recall, F1-score e matriz de confus\u00e3o. A acur\u00e1cia indica a propor\u00e7\u00e3o de previs\u00f5es corretas, enquanto precis\u00e3o, recall e F1-score fornecem uma vis\u00e3o mais detalhada do desempenho em cada classe. A matriz de confus\u00e3o mostra os acertos e erros do modelo Decision Tree, permitindo identificar onde o modelo est\u00e1 tendo dificuldades, como confundir pessoas \"fit\" com \"not fit\".</p> <p>Exemplo de c\u00f3digo: </p><pre><code>from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n\ny_pred = clf_viz.predict(x_test)\nacc = accuracy_score(y_test, y_pred)\nprec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\ncm = confusion_matrix(y_test, y_pred)\n</code></pre><p></p> <p></p> <p>A Decision Tree teve bom desempenho, mas ainda comete erros, especialmente devido ao desbalanceamento das classes. A matriz de confus\u00e3o e as m\u00e9tricas ajudam a identificar oportunidades de melhoria, como ajuste de hiperpar\u00e2metros ou t\u00e9cnicas de balanceamento.</p>"},{"location":"arvore-decisao/main/#6-relatorio-final","title":"6. Relat\u00f3rio Final","text":"<p>Todo o processo foi documentado, incluindo as etapas de explora\u00e7\u00e3o, pr\u00e9-processamento, divis\u00e3o dos dados, treinamento e avalia\u00e7\u00e3o do modelo. Os resultados obtidos indicam que o modelo consegue distinguir quem est\u00e1 \"fit\" e \"not fit\", mas ainda h\u00e1 espa\u00e7o para melhorias, como ajuste de hiperpar\u00e2metros, t\u00e9cnicas de balanceamento das classes e inclus\u00e3o de novas vari\u00e1veis relevantes. A an\u00e1lise refor\u00e7a que h\u00e1bitos de vida, como atividade f\u00edsica, tabagismo, peso e nutri\u00e7\u00e3o, s\u00e3o determinantes para o condicionamento f\u00edsico.</p>"},{"location":"projeto/main/","title":"Projeto","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Aqui vai o objetivo macro do roteiro. Por que estamos fazendo o que estamos fazendo?</p>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Instalando o MAAS:</p> sudo snap install maas --channel=3.5/Stable <p></p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro1/main/#app","title":"App","text":""},{"location":"roteiro1/main/#tarefa-1_1","title":"Tarefa 1","text":""},{"location":"roteiro1/main/#tarefa-2_1","title":"Tarefa 2","text":"<p>Exemplo de diagrama</p> <pre><code>architecture-beta\n    group api(cloud)[API]\n\n    service db(database)[Database] in api\n    service disk1(disk)[Storage] in api\n    service disk2(disk)[Storage] in api\n    service server(server)[Server] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n    disk2:T -- B:db</code></pre> <p>Mermaid</p>"},{"location":"roteiro1/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"roteiro2/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> </p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> <p></p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> 2025-12-08T02:07:57.705569 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ 2025-12-08T02:07:59.241768 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"}]}